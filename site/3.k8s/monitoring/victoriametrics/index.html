
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Project documentation with Markdown.">
      
      
        <meta name="author" content="Li">
      
      
        <link rel="canonical" href="https://barry-boy.github.io/3.k8s/monitoring/victoriametrics/">
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-8.0.0">
    
    
      
        <title>VictoriaMetrics - Kubernetes 进阶训练营</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.ad626c1e.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.9204c3b2.min.css">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,400i,700%7CUbuntu+Mono&display=fallback">
        <style>:root{--md-text-font:"Ubuntu";--md-code-font:"Ubuntu Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../stylesheets/extra.css">
    
    <script>function __md_scope(e,t,_){return new URL(_||(t===localStorage?"../../..":"../../.."),location).pathname+"."+e}function __md_get(e,t=localStorage,_){return JSON.parse(t.getItem(__md_scope(e,t,_)))}function __md_set(e,t,_=localStorage,o){try{_.setItem(__md_scope(e,_,o),JSON.stringify(t))}catch(e){}}</script>
    
      


    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#victoriametrics" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../../.." title="Kubernetes 进阶训练营" class="md-header__button md-logo" aria-label="Kubernetes 进阶训练营" data-md-component="logo">
      
  <img src="../../../material/logo.jpg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Kubernetes 进阶训练营
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              VictoriaMetrics
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="切换暗色主题"  type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="切换暗色主题" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
            </label>
          
        
          
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="blue" data-md-color-accent="blue"  aria-label="切换亮色主题"  type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="切换亮色主题" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3z"/></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="清空当前内容" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/barry-boy/barry-boy.github.io" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href="../../.." class="md-tabs__link">
      首页
    </a>
  </li>

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../0.Internet/Tcp-ip-docs/" class="md-tabs__link">
        0、网络基础
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../1.os/ubuntu/kjc-doc/" class="md-tabs__link">
        1、Linux
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../basic/cncf-docs/" class="md-tabs__link">
        2、kubernetes
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../python/1-python-docs/" class="md-tabs__link">
        3、运维开发
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../k8s/mysql-operator/0-mysql-docs.md" class="md-tabs__link">
        4、数据库
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../5.Storage/disk-ssd-docs/" class="md-tabs__link">
        5、存储服务
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../6.Gitops/2024-9-26-Git-docs/" class="md-tabs__link">
        6、Gitops
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../Devops/git-error-docs/" class="md-tabs__link">
        7、Devops
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../ansible/ansible-case1/" class="md-tabs__link">
        8、运维自动化
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../Macuse/mac-kubeconfig/" class="md-tabs__link">
        9、其他
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../chatgpt/chatgpt-docs/" class="md-tabs__link">
        10、Chatgpt
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../20.mylife/2024-9-24-myblog-life-docs/" class="md-tabs__link">
        关于我
      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Kubernetes 进阶训练营" class="md-nav__button md-logo" aria-label="Kubernetes 进阶训练营" data-md-component="logo">
      
  <img src="../../../material/logo.jpg" alt="logo">

    </a>
    Kubernetes 进阶训练营
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/barry-boy/barry-boy.github.io" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        首页
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" data-md-state="indeterminate" type="checkbox" id="__nav_2" checked>
      
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_2">
          0、网络基础
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="0、网络基础" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          0、网络基础
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.Internet/Tcp-ip-docs/" class="md-nav__link">
        网络 Tcp/IP
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.Internet/5g-docs/" class="md-nav__link">
        网络 思科设备
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.Internet/IB-ET-docs/" class="md-nav__link">
        网络 网卡模式调整
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.os/asus/asus-docs/" class="md-nav__link">
        华硕服务器
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.Internet/ZeroTier/" class="md-nav__link">
        ZeroTier组网
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.Internet/frp/" class="md-nav__link">
        frp 内网穿透
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.Internet/3-0.Internet-docs.md" class="md-nav__link">
        kubernetes 网络组件
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.Internet/4-network-docs/" class="md-nav__link">
        科学上网
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" data-md-state="indeterminate" type="checkbox" id="__nav_3" checked>
      
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3">
          1、Linux
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="1、Linux" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          1、Linux
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.os/ubuntu/kjc-doc/" class="md-nav__link">
        Linux 云计算手册大纲
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.os/hardware/cpu-doc/" class="md-nav__link">
        Linux 硬件 CPU
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.os/ubuntu/ubuntu-bmc-doc/" class="md-nav__link">
        Linux 带外管理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.os/ubuntu/ubuntu-install-doc/" class="md-nav__link">
        Linux 系统安装
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.os/ubuntu/ubuntu-cli-doc/" class="md-nav__link">
        Linux 系统命令
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.os/ubuntu/ubuntu-mirror-doc/" class="md-nav__link">
        Linux 国内apt源
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.os/ubuntu/ubuntu-nvme-doc/" class="md-nav__link">
        Linux nvme磁盘管理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.os/ubuntu/ubuntu-parted-doc/" class="md-nav__link">
        Linux parted磁盘管理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.os/ubuntu/ubuntu-reboot-doc/" class="md-nav__link">
        Linux 系统强制重启
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.os/ubuntu/ubuntu-disk-doc/" class="md-nav__link">
        Linux 磁盘管理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.os/ubuntu/ubuntu-network-doc/" class="md-nav__link">
        Linux 网络服务
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.os/2024-9-25-ssh-proxy-docs/" class="md-nav__link">
        Linux SSH 转发代理实现
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" data-md-state="indeterminate" type="checkbox" id="__nav_4" checked>
      
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_4">
          2、kubernetes
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="2、kubernetes" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          2、kubernetes
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../basic/cncf-docs/" class="md-nav__link">
        云原生简介及CNCF
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../docker/docker-docs/" class="md-nav__link">
        Docker 基本管理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../docker/lxcfs.md" class="md-nav__link">
        lxcfs 介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../docker/docker-images-docs/" class="md-nav__link">
        docker 镜像构建
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../docker/docker-proxy-docs/" class="md-nav__link">
        docker proxy代理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../docker/docker-image-clash-docs/" class="md-nav__link">
        docker Clash代理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../base/k8s-overview/" class="md-nav__link">
        kubernetes 简介
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../3-k8s-systeminit-docs/" class="md-nav__link">
        kubernetes 集群部署
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../2024-9-19-k8s-update/" class="md-nav__link">
        kubernetes 集群升级
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../pod-base-docs/" class="md-nav__link">
        kubernetes pod 基础原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../pod-hook-docs/" class="md-nav__link">
        kubernetes pod 生命周期
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../basic/6-kubernetes-etcd-docs/" class="md-nav__link">
        kubernetes etcd 简介与实践
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../basic/7-kubernetes-coredns-docs/" class="md-nav__link">
        kubernetes coredns 简介与实践
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../k8s/k8s-calico-docs.md" class="md-nav__link">
        kubernetes calico 网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../yaml/yaml/" class="md-nav__link">
        kubernetes 资源清单
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../base/k8s-limit-docs/" class="md-nav__link">
        kubernetes 资源限制
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../k8s-configmap-docs/" class="md-nav__link">
        kubernetes 配置管理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../basic/5-kubernetes-UI-docs/" class="md-nav__link">
        Kubernetes UI
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../deploy-sts-ds-docs/" class="md-nav__link">
        kubernetes 控制器
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../network/ingress-%E6%B5%81%E9%87%8F/" class="md-nav__link">
        Ingress 流量
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../network/%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0/" class="md-nav__link">
        服务发现
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../network/gateway-api/" class="md-nav__link">
        Gateway API
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../network/dns-%E4%BC%98%E5%8C%96/" class="md-nav__link">
        DNS 优化
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../network/ingress-nginx/" class="md-nav__link">
        ingress-nginx
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../network/service-profiles/" class="md-nav__link">
        Service Profiles
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../network/4-network-docs.md" class="md-nav__link">
        科学上网
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../5.Storage/pv-rep/" class="md-nav__link">
        kubernetes pv/pvc简介
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../k8s-mon-docs/" class="md-nav__link">
        kubernetes 监控服务
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../k8s-logs-docs/" class="md-nav__link">
        kubernetes 日志管理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../k8s-istio-docs/" class="md-nav__link">
        kubernetes 服务网格
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../cks/cka/" class="md-nav__link">
        kubernetes CKA考题
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../cks/cks/" class="md-nav__link">
        kubernetes CKS考题
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../ui/rancher-update-docs/" class="md-nav__link">
        kubernetes Rancher 升级
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../basic/7-kubernetes-update-docs/" class="md-nav__link">
        kubernetes 节点维护
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Helm/helm-install/" class="md-nav__link">
        kubernetes Helm 实践
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Helm/helm-chart/" class="md-nav__link">
        kubernetes Helm chart包
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../2024-9-24-helmfile-docs/" class="md-nav__link">
        kubernetes Helmfile 文件
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../cert-manager/cert-manager-doc/" class="md-nav__link">
        kubernetes Cert-manager 安装
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../other/k8s-ca-docs/" class="md-nav__link">
        kubernetes 处理k8s证书过期
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../7.logging/%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86%E6%9E%B6%E6%9E%84/" class="md-nav__link">
        kubernete 日志架构
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../7.logging/%E6%90%AD%E5%BB%BA-efk-%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F/" class="md-nav__link">
        kubernetes EFK日志系统
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../docs/3.k8s/7.logging/fluentd.md" class="md-nav__link">
        kubernete 日志fluentd
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../7.logging/loki/" class="md-nav__link">
        kubernete Loki
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../7.logging/%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB%E6%A8%A1%E5%BC%8F/" class="md-nav__link">
        kubernetes 日志读写分离模式
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../7.logging/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%A8%A1%E5%BC%8F/" class="md-nav__link">
        kubernetes 日志微服务模式
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../7.logging/%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/" class="md-nav__link">
        kubernetes 日志配置文件
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../7.logging/%E6%97%A5%E5%BF%97%E6%8A%A5%E8%AD%A6/" class="md-nav__link">
        kubernetes 日志报警
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../7.logging/logql/" class="md-nav__link">
        kubernetes 日志 Logql
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../ansible-install-k8s-docs/" class="md-nav__link">
        kubernetes 自动化安装
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../4-ansible-install-k8s/" class="md-nav__link">
        kubernetes 项目一
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../basic/9-install-keepalived-cluster/" class="md-nav__link">
        kubernetes 项目二
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" data-md-state="indeterminate" type="checkbox" id="__nav_5" checked>
      
      
      
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5">
          3、运维开发
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="3、运维开发" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          3、运维开发
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../python/1-python-docs/" class="md-nav__link">
        Python 开发-基础语法
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../python/2-python-docs/" class="md-nav__link">
        Python 开发-用户交互
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../go/go-base-docs/" class="md-nav__link">
        GO开发-环境安装
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../go/2-go-docs/" class="md-nav__link">
        GO 开发-操作符和表达式
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../python/module-docs/" class="md-nav__link">
        模块
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6" data-md-state="indeterminate" type="checkbox" id="__nav_6" checked>
      
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_6">
          4、数据库
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="4、数据库" data-md-level="1">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          4、数据库
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../k8s/mysql-operator/0-mysql-docs.md" class="md-nav__link">
        Mysql 数据库
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../k8s/mysql-operator/1-install-mysql-operator.md" class="md-nav__link">
        Mysql operator
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../k8s/mysql-operator/2-mysql-backup-docs.md" class="md-nav__link">
        Mysql 备份恢复
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../k8s/mysql-operator/3-mysql-backup-error-docs.md" class="md-nav__link">
        数据库问题处理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../redis/redis-class-docs/" class="md-nav__link">
        redis 大纲
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../5.Storage/redis-docs/" class="md-nav__link">
        redis 缓存加速
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../5.Storage/redis-cluster-docs/" class="md-nav__link">
        redis 集群篇
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../redis/redis-save-docs/" class="md-nav__link">
        redis 持久化
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_7" data-md-state="indeterminate" type="checkbox" id="__nav_7" checked>
      
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_7">
          5、存储服务
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="5、存储服务" data-md-level="1">
        <label class="md-nav__title" for="__nav_7">
          <span class="md-nav__icon md-icon"></span>
          5、存储服务
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../5.Storage/disk-ssd-docs/" class="md-nav__link">
        存储分类
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../5.Storage/ceph-base/" class="md-nav__link">
        ceph 基础
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../5.Storage/rook-ceph/" class="md-nav__link">
        ceph 分布式存储
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../5.Storage/rbd-docs/" class="md-nav__link">
        ceph RBD使用
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../5.Storage/cephfs-docs/" class="md-nav__link">
        ceph 文件系统使用
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../5.Storage/ceph-cli-docs/" class="md-nav__link">
        ceph 命令用法
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../5.Storage/osd/" class="md-nav__link">
        ceph 性能测试
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../5.Storage/ceph-bucket/" class="md-nav__link">
        ceph 对象存储
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../5.Storage/s3-client-docs/" class="md-nav__link">
        ceph s3cmd管理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../5.Storage/5-dev-docs/" class="md-nav__link">
        ceph 存储修复
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../5.Storage/expand-docs/" class="md-nav__link">
        ceph 存储扩容
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../5.Storage/ceph-tools/" class="md-nav__link">
        ceph 外部环境连接集群
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../5.Storage/clean-ceph-docs/" class="md-nav__link">
        ceph 集群清理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../5.Storage/ceph-error/" class="md-nav__link">
        ceph 错误指南
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../5.Storage/6-ceph-crush-docs/" class="md-nav__link">
        ceph 高级参数
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../5.Storage/Juicefs/" class="md-nav__link">
        Juicefs存储方案
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../5.Storage/juicefs-update/" class="md-nav__link">
        Juicefs版本升级
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_8" data-md-state="indeterminate" type="checkbox" id="__nav_8" checked>
      
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_8">
          6、Gitops
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="6、Gitops" data-md-level="1">
        <label class="md-nav__title" for="__nav_8">
          <span class="md-nav__icon md-icon"></span>
          6、Gitops
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../6.Gitops/2024-9-26-Git-docs/" class="md-nav__link">
        Git Commit 问题
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../6.Gitops/2024-10-10-Github-rsync-Gitee/" class="md-nav__link">
        Github Rysnc 同步 Gitee
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_9" data-md-state="indeterminate" type="checkbox" id="__nav_9" checked>
      
      
      
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_9">
          7、Devops
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="7、Devops" data-md-level="1">
        <label class="md-nav__title" for="__nav_9">
          <span class="md-nav__icon md-icon"></span>
          7、Devops
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Devops/git-error-docs/" class="md-nav__link">
        Git 报错指南
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Devops/github/" class="md-nav__link">
        Github
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Devops/4-github-gitee-doc/" class="md-nav__link">
        Github仓库同步至Gitee
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Devops/3-github-docs/" class="md-nav__link">
        Google 浏览器扩展
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_10" data-md-state="indeterminate" type="checkbox" id="__nav_10" checked>
      
      
      
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_10">
          8、运维自动化
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="8、运维自动化" data-md-level="1">
        <label class="md-nav__title" for="__nav_10">
          <span class="md-nav__icon md-icon"></span>
          8、运维自动化
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../ansible/ansible-case1/" class="md-nav__link">
        Ansible 应用基础
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../ansible/ansible-docs2/" class="md-nav__link">
        Ansible 高级用法
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../ansible/ansible-install-docs/" class="md-nav__link">
        Ansible 安装
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../ansible/ansible-playbook/" class="md-nav__link">
        Ansible playbook
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_11" data-md-state="indeterminate" type="checkbox" id="__nav_11" checked>
      
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_11">
          9、其他
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="9、其他" data-md-level="1">
        <label class="md-nav__title" for="__nav_11">
          <span class="md-nav__icon md-icon"></span>
          9、其他
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Macuse/mac-kubeconfig/" class="md-nav__link">
        kubeconfig 实践应用
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../k3s/k3s-docs/" class="md-nav__link">
        k3s 大纲介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../k3s/k3s-install/" class="md-nav__link">
        k3s 边缘计算
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Macuse/mac-k9s-doc/" class="md-nav__link">
        k9s 实践应用
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Macuse/mac-usb-system/" class="md-nav__link">
        Mac 制作ubuntu系统盘
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Macuse/mac-vscode/" class="md-nav__link">
        Mac vscode使用技巧
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../error/tjtu/" class="md-nav__link">
        天津集群
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../error/dev/" class="md-nav__link">
        Dev环境集群
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../error/delete-prometheus-db/" class="md-nav__link">
        误删除监控数据
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../error/kubernetes-pvc-ubound-doc/" class="md-nav__link">
        Kubernetes pvc ubound
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../error/cent1.os-dns-error-docs.md" class="md-nav__link">
        dns 文件权限调整
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../other/mac-docs/" class="md-nav__link">
        如何利用Mac 写博客文章
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../other/google-docs/" class="md-nav__link">
        高效利用谷歌来搜索
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../other/discord-docs/" class="md-nav__link">
        Discord 新手教程
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_12" data-md-state="indeterminate" type="checkbox" id="__nav_12" checked>
      
      
      
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_12">
          10、Chatgpt
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="10、Chatgpt" data-md-level="1">
        <label class="md-nav__title" for="__nav_12">
          <span class="md-nav__icon md-icon"></span>
          10、Chatgpt
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../chatgpt/chatgpt-docs/" class="md-nav__link">
        注册chatgpt
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../chatgpt/google-ai-docs/" class="md-nav__link">
        搜索技术记录
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../chatgpt/gpts-docs/" class="md-nav__link">
        如何使用Chatgpt来制作专属gpt
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_13" data-md-state="indeterminate" type="checkbox" id="__nav_13" checked>
      
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_13">
          关于我
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="关于我" data-md-level="1">
        <label class="md-nav__title" for="__nav_13">
          <span class="md-nav__icon md-icon"></span>
          关于我
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../20.mylife/2024-9-24-myblog-life-docs/" class="md-nav__link">
        我的博客
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../20.mylife/2021-life-docs/" class="md-nav__link">
        我的2021
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../20.mylife/2022-life-docs/" class="md-nav__link">
        我的2022
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../20.mylife/2023-life-docs/" class="md-nav__link">
        我的2023
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../20.mylife/2024-life-docs/" class="md-nav__link">
        我的2024
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../20.mylife/2025-learning-docs/" class="md-nav__link">
        我的学习
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
<a href="https://github.com/barry-boy/barry-boy.github.io/edit/master/docs/3.k8s/monitoring/victoriametrics.md" title="编辑此页" class="md-content__button md-icon">
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
</a>


<h1 id="victoriametrics">VictoriaMetrics<a class="headerlink" href="#victoriametrics" title="Permanent link">&para;</a></h1>
<p><a href="https://github.com/cnych/qikqiak.com/edit/master/docs/monitor/victorialmetrics.md" title="编辑此页"> </a></p>
<h1 id="victoriametrics_1">VictoriaMetrics<a class="headerlink" href="#victoriametrics_1" title="Permanent link">&para;</a></h1>
<p><a href="https://victoriametrics.com/">VictoriaMetrics(VM)</a> 是一个支持高可用、经济高效且可扩展的监控解决方案和时间序列数据库，可用于 Prometheus 监控数据做长期远程存储。</p>
<p>前面章节我们介绍了 Thanos 方案也可以用来解决 Prometheus 的高可用和远程存储的问题，那么为什么我们还要使用 VictoriaMetrics 呢？相对于 Thanos，VictoriaMetrics 主要是一个可水平扩容的<strong>本地全量持久化存储方案</strong> ，VictoriaMetrics 不仅仅是时序数据库，它的优势主要体现在一下几点。</p>
<ul>
<li>对外支持 Prometheus 相关的 API，可以直接用于 Grafana 作为 Prometheus 数据源使用</li>
<li>指标数据摄取和查询具备高性能和良好的可扩展性，性能比 InfluxDB 和 TimescaleDB 高出 20 倍</li>
<li>在处理高基数时间序列时，内存方面也做了优化，比 InfluxDB 少 10x 倍，比 Prometheus、Thanos 或 Cortex 少 7 倍</li>
<li>高性能的数据压缩方式，与 TimescaleDB 相比，可以将多达 70 倍的数据点存入有限的存储空间，与 Prometheus、Thanos 或 Cortex 相比，所需的存储空间减少 7 倍</li>
<li>它针对具有高延迟 IO 和低 IOPS 的存储进行了优化</li>
<li>提供全局的查询视图，多个 Prometheus 实例或任何其他数据源可能会将数据摄取到 VictoriaMetrics</li>
<li>
<p>操作简单</p>
</li>
<li>
<p>VictoriaMetrics 由一个没有外部依赖的小型可执行文件组成</p>
</li>
<li>
<p>所有的配置都是通过明确的命令行标志和合理的默认值完成的</p>
</li>
<li>所有数据都存储在 - storageDataPath 命令行参数指向的目录中</li>
<li>
<p>可以使用 <code>vmbackup/vmrestore</code> 工具轻松快速地从实时快照备份到 S3 或 GCS 对象存储中</p>
</li>
<li>
<p>支持从第三方时序数据库获取数据源</p>
</li>
<li>
<p>由于存储架构，它可以保护存储在非正常关机（即 OOM、硬件重置或 kill -9）时免受数据损坏</p>
</li>
<li>同样支持指标的 relabel 操作</li>
</ul>
<h2 id="_1">架构<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h2>
<p>VM 分为单节点和集群两个方案，根据业务需求选择即可。单节点版直接运行一个二进制文件既，官方建议采集数据点(data points)低于 100w/s，推荐 VM 单节点版，简单好维护，但不支持告警。集群版支持数据水平拆分。下图是 <code>VictoriaMetrics</code> 集群版官方的架构图。</p>
<p><img alt="VM官方架构" src="https://picdn.youdianzhishi.com/images/1650529246872.jpg" /></p>
<p>主要包含以下几个组件：</p>
<ul>
<li><code>vmstorage</code>：数据存储以及查询结果返回，默认端口为 8482</li>
<li><code>vminsert</code>：数据录入，可实现类似分片、副本功能，默认端口 8480</li>
<li><code>vmselect</code>：数据查询，汇总和数据去重，默认端口 8481</li>
<li><code>vmagent</code>：数据指标抓取，支持多种后端存储，会占用本地磁盘缓存，默认端口 8429</li>
<li><code>vmalert</code>：报警相关组件，不如果不需要告警功能可以不使用该组件，默认端口为 8880</li>
</ul>
<p>集群方案把功能拆分为 vmstorage、 vminsert、vmselect 组件，如果要替换 Prometheus，还需要使用 vmagent、vmalert。从上图也可以看出 vminsert 以及 vmselect 都是无状态的，所以扩展很简单，只有 vmstorage 是有状态的。</p>
<p><code>vmagent</code> 的主要目的是用来收集指标数据然后存储到 VM 以及 Prometheus 兼容的存储系统中（支持 remote_write 协议即可）。</p>
<p>下图是 vmagent 的一个简单架构图，可以看出该组件也实现了 metrics 的 push 功能，此外还有很多其他特性：</p>
<ul>
<li>替换 prometheus 的 scraping target</li>
<li>支持基于 prometheus relabeling 的模式添加、移除、修改 labels，可以方便在数据发送到远端存储之前进行数据的过滤</li>
<li>支持多种数据协议，influx line 协议，graphite 文本协议，opentsdb 协议，prometheus remote write 协议，json lines 协议，csv 数据</li>
<li>支持收集数据的同时，并复制到多种远端存储系统</li>
<li>支持不可靠远端存储（通过本地存储 <code>-remoteWrite.tmpDataPath</code> )，同时支持最大磁盘占用</li>
<li>相比 prometheus 使用较少的内存、cpu、磁盘 io 以及网络带宽</li>
</ul>
<p><img alt="vmagent" src="https://picdn.youdianzhishi.com/images/1650529595671.jpg" /></p>
<p>接下来我们就分别来介绍了 VM 的单节点和集群两个方案的使用。</p>
<h2 id="_2">单节点<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h2>
<p>这里我们采集 node-exporter 为例进行说明，首先使用 Prometheus 采集数据，然后将 Prometheus 数据远程写入 VM 远程存储，由于 VM 提供了 <code>vmagent</code> 组件，最后我们使用 VM 来完全替换 Prometheus，可以使架构更简单、更低的资源占用。</p>
<p>这里我们将所有资源运行在 <code>kube-vm</code> 命名空间之下：</p>
<div class="highlight"><pre><span></span><code>☸ ➜ kubectl create ns kube-vm
</code></pre></div>
<p>首先我们这 <code>kube-vm</code> 命名空间下面使用 DaemonSet 控制器运行 node-exporter，对应的资源清单文件如下所示：</p>
<div class="highlight"><pre><span></span><code># vm-node-exporter.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-exporter
  namespace: kube-vm
spec:
  selector:
    matchLabels:
      app: node-exporter
  template:
    metadata:
      labels:
        app: node-exporter
    spec:
      hostPID: true
      hostIPC: true
      hostNetwork: true
      nodeSelector:
        kubernetes.io/os: linux
      containers:
        - name: node-exporter
          image: prom/node-exporter:v1.3.1
          args:
            - --web.listen-address=$(HOSTIP):9111
            - --path.procfs=/host/proc
            - --path.sysfs=/host/sys
            - --path.rootfs=/host/root
            - --no-collector.hwmon # 禁用不需要的一些采集器
            - --no-collector.nfs
            - --no-collector.nfsd
            - --no-collector.nvme
            - --no-collector.dmi
            - --no-collector.arp
            - --collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/containerd/.+|/var/lib/docker/.+|var/lib/kubelet/pods/.+)($|/)
            - --collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|cgroup|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|mqueue|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|sysfs|tracefs)$
          ports:
            - containerPort: 9111
          env:
            - name: HOSTIP
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
          resources:
            requests:
              cpu: 150m
              memory: 180Mi
            limits:
              cpu: 150m
              memory: 180Mi
          securityContext:
            runAsNonRoot: true
            runAsUser: 65534
          volumeMounts:
            - name: proc
              mountPath: /host/proc
            - name: sys
              mountPath: /host/sys
            - name: root
              mountPath: /host/root
              mountPropagation: HostToContainer
              readOnly: true
      tolerations: # 添加容忍
        - operator: &quot;Exists&quot;
      volumes:
        - name: proc
          hostPath:
            path: /proc
        - name: dev
          hostPath:
            path: /dev
        - name: sys
          hostPath:
            path: /sys
        - name: root
          hostPath:
            path: /
</code></pre></div>
<p>由于前面章节中我们也创建了 node-exporter，为了防止端口冲突，这里我们使用参数 <code>--web.listen-address=$(HOSTIP):9111</code> 配置端口为 <code>9111</code>。直接应用上面的资源清单即可。</p>
<div class="highlight"><pre><span></span><code>☸ ➜ kubectl apply -f vm-node-exporter.yaml
☸ ➜ kubectl get pods -n kube-vm -owide
NAME                  READY   STATUS    RESTARTS   AGE    IP              NODE      NOMINATED NODE   READINESS GATES
node-exporter-c4d76   1/1     Running   0          118s   192.168.0.109   node2     &lt;none&gt;           &lt;none&gt;
node-exporter-hzt8s   1/1     Running   0          118s   192.168.0.111   master1   &lt;none&gt;           &lt;none&gt;
node-exporter-zlxwb   1/1     Running   0          118s   192.168.0.110   node1     &lt;none&gt;           &lt;none&gt;
</code></pre></div>
<p>然后重新部署一套独立的 Prometheus，为了简单我们直接使用 <code>static_configs</code> 静态配置方式来抓取 node-exporter 的指标，配置清单如下所示：</p>
<div class="highlight"><pre><span></span><code># vm-prom-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: kube-vm
data:
  prometheus.yaml: |
    global:
      scrape_interval: 15s
      scrape_timeout: 15s
    scrape_configs:
    - job_name: &quot;nodes&quot;
      static_configs:
      - targets: [&#39;192.168.0.109:9111&#39;, &#39;192.168.0.110:9111&#39;, &#39;192.168.0.111:9111&#39;]
      relabel_configs: # 通过 relabeling 从 __address__ 中提取 IP 信息，为了后面验证 VM 是否兼容 relabeling
      - source_labels: [__address__]
        regex: &quot;(.*):(.*)&quot;
        replacement: &quot;${1}&quot;
        target_label: &#39;ip&#39;
        action: replace
</code></pre></div>
<p>上面配置中通过 <code>relabel</code> 操作从 <code>__address__</code> 中将 IP 信息提取出来，后面可以用来验证 VM 是否兼容 <code>relabel</code> 操作。</p>
<p>同样要给 Prometheus 数据做持久化，所以也需要创建一个对应的 PVC 资源对象：</p>
<div class="highlight"><pre><span></span><code># apiVersion: storage.k8s.io/v1
# kind: StorageClass
# metadata:
#   name: local-storage
# provisioner: kubernetes.io/no-provisioner
# volumeBindingMode: WaitForFirstConsumer
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: prometheus-data
spec:
  accessModes:
    - ReadWriteOnce
  capacity:
    storage: 20Gi
  storageClassName: local-storage
  local:
    path: /data/k8s/prometheus
  persistentVolumeReclaimPolicy: Retain
  nodeAffinity:
    required:
      nodeSelectorTerms:
        - matchExpressions:
            - key: kubernetes.io/hostname
              operator: In
              values:
                - node2
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: prometheus-data
  namespace: kube-vm
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi
  storageClassName: local-storage
</code></pre></div>
<p>然后直接创建 Prometheus 即可，将上面的 PVC 和 ConfigMap 挂载到容器中，通过 <code>--config.file</code> 参数指定配置文件文件路径，指定 TSDB 数据路径等，资源清单文件如下所示：</p>
<div class="highlight"><pre><span></span><code># vm-prom-deploy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus
  namespace: kube-vm
spec:
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: prometheus-data
        - name: config-volume
          configMap:
            name: prometheus-config
      containers:
        - image: prom/prometheus:v2.35.0
          name: prometheus
          args:
            - &quot;--config.file=/etc/prometheus/prometheus.yaml&quot;
            - &quot;--storage.tsdb.path=/prometheus&quot; # 指定tsdb数据路径
            - &quot;--storage.tsdb.retention.time=2d&quot;
            - &quot;--web.enable-lifecycle&quot; # 支持热更新，直接执行localhost:9090/-/reload立即生效
          ports:
            - containerPort: 9090
              name: http
          securityContext:
            runAsUser: 0
          volumeMounts:
            - mountPath: &quot;/etc/prometheus&quot;
              name: config-volume
            - mountPath: &quot;/prometheus&quot;
              name: data
---
apiVersion: v1
kind: Service
metadata:
  name: prometheus
  namespace: kube-vm
spec:
  selector:
    app: prometheus
  type: NodePort
  ports:
    - name: web
      port: 9090
      targetPort: http
</code></pre></div>
<p>直接应用上面的资源清单即可。</p>
<div class="highlight"><pre><span></span><code>☸ ➜ kubectl apply -f vm-prom-config.yaml
☸ ➜ kubectl apply -f vm-prom-pvc.yaml
☸ ➜ kubectl apply -f vm-prom-deploy.yaml
☸ ➜ kubectl get pods -n kube-vm -owide
NAME                      READY   STATUS    RESTARTS   AGE     IP              NODE      NOMINATED NODE   READINESS GATES
node-exporter-c4d76       1/1     Running   0          27m     192.168.0.109   node2     &lt;none&gt;           &lt;none&gt;
node-exporter-hzt8s       1/1     Running   0          27m     192.168.0.111   master1   &lt;none&gt;           &lt;none&gt;
node-exporter-zlxwb       1/1     Running   0          27m     192.168.0.110   node1     &lt;none&gt;           &lt;none&gt;
prometheus-dfc9f6-2w2vf   1/1     Running   0          4m58s   10.244.2.102    node2     &lt;none&gt;           &lt;none&gt;
☸ ➜ kubectl get svc -n kube-vm
NAME         TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
prometheus   NodePort   10.103.38.114   &lt;none&gt;        9090:31890/TCP   4m10s
</code></pre></div>
<p>部署完成后可以通过 <code>http://&lt;node-ip&gt;:31890</code> 访问 Prometheus，正常可以看到采集的 3 个 node 节点的指标任务。</p>
<p><img alt="nodes targets" src="https://picdn.youdianzhishi.com/images/1650615524453.png" /></p>
<p>同样的方式重新部署 Grafana，资源清单如下所示：</p>
<div class="highlight"><pre><span></span><code># vm-grafana.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  namespace: kube-vm
spec:
  selector:
    matchLabels:
      app: grafana
  template:
    metadata:
      labels:
        app: grafana
    spec:
      volumes:
        - name: storage
          persistentVolumeClaim:
            claimName: grafana-data
      containers:
        - name: grafana
          image: grafana/grafana:main
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 3000
              name: grafana
          securityContext:
            runAsUser: 0
          env:
            - name: GF_SECURITY_ADMIN_USER
              value: admin
            - name: GF_SECURITY_ADMIN_PASSWORD
              value: admin321
          volumeMounts:
            - mountPath: /var/lib/grafana
              name: storage
---
apiVersion: v1
kind: Service
metadata:
  name: grafana
  namespace: kube-vm
spec:
  type: NodePort
  ports:
    - port: 3000
  selector:
    app: grafana
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: grafana-data
spec:
  accessModes:
    - ReadWriteOnce
  capacity:
    storage: 1Gi
  storageClassName: local-storage
  local:
    path: /data/k8s/grafana
  persistentVolumeReclaimPolicy: Retain
  nodeAffinity:
    required:
      nodeSelectorTerms:
        - matchExpressions:
            - key: kubernetes.io/hostname
              operator: In
              values:
                - node2
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: grafana-data
  namespace: kube-vm
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: local-storage



☸ ➜ kubectl apply -f vm-grafana.yaml
☸ ➜ kubectl get svc -n kube-vm |grep grafana
grafana      NodePort   10.97.111.153   &lt;none&gt;        3000:31800/TCP   62s
</code></pre></div>
<p>同样通过 <code>http://&lt;node-ip&gt;:31800</code> 就可以访问 Grafana 了，进入 Grafana 配置 Prometheus 数据源。</p>
<p><img alt="Grafana 数据源" src="https://picdn.youdianzhishi.com/images/1650616039116.png" /></p>
<p>然后导入 <a href="https://grafana.com/grafana/dashboards/16098">16098</a> 这个 Dashboard，导入后效果如下图所示。</p>
<p><img alt="node exporter dashboard" src="https://picdn.youdianzhishi.com/images/1650616895168.png" /></p>
<p>到这里就完成了使用 Prometheus 收集节点监控指标，接下来我们来使用 VM 来改造现有方案。</p>
<h3 id="victoriametrics_2">远程存储 VictoriaMetrics<a class="headerlink" href="#victoriametrics_2" title="Permanent link">&para;</a></h3>
<p>首先需要一个单节点模式的 VM，运行 VM 很简单，可以直接下载对应的二进制文件启动，也可以使用 docker 镜像一键启动，我们这里同样部署到 Kubernetes 集群中。资源清单文件如下所示。</p>
<div class="highlight"><pre><span></span><code># vm-grafana.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: victoria-metrics
  namespace: kube-vm
spec:
  selector:
    matchLabels:
      app: victoria-metrics
  template:
    metadata:
      labels:
        app: victoria-metrics
    spec:
      volumes:
        - name: storage
          persistentVolumeClaim:
            claimName: victoria-metrics-data
      containers:
        - name: vm
          image: victoriametrics/victoria-metrics:v1.76.1
          imagePullPolicy: IfNotPresent
          args:
            - -storageDataPath=/var/lib/victoria-metrics-data
            - -retentionPeriod=1w
          ports:
            - containerPort: 8428
              name: http
          volumeMounts:
            - mountPath: /var/lib/victoria-metrics-data
              name: storage
---
apiVersion: v1
kind: Service
metadata:
  name: victoria-metrics
  namespace: kube-vm
spec:
  type: NodePort
  ports:
    - port: 8428
  selector:
    app: victoria-metrics
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: victoria-metrics-data
spec:
  accessModes:
    - ReadWriteOnce
  capacity:
    storage: 20Gi
  storageClassName: local-storage
  local:
    path: /data/k8s/vm
  persistentVolumeReclaimPolicy: Retain
  nodeAffinity:
    required:
      nodeSelectorTerms:
        - matchExpressions:
            - key: kubernetes.io/hostname
              operator: In
              values:
                - node2
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: victoria-metrics-data
  namespace: kube-vm
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi
  storageClassName: local-storage
</code></pre></div>
<p>这里我们使用 <code>-storageDataPath</code> 参数指定了数据存储目录，然后同样将该目录进行了持久化，<code>-retentionPeriod</code> 参数可以用来配置数据的保持周期。直接应用上面的资源清单即可。</p>
<div class="highlight"><pre><span></span><code>☸ ➜ kubectl apply -f vm-single-node-deploy.yaml
☸ ➜ kubectl get svc victoria-metrics -n kube-vm
NAME               TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
victoria-metrics   NodePort   10.106.216.248   &lt;none&gt;        8428:31953/TCP   75m
☸ ➜ kubectl get pods -n kube-vm -l app=victoria-metrics
NAME                                READY   STATUS    RESTARTS   AGE
victoria-metrics-57d47f4587-htb88   1/1     Running   0          3m12s
☸ ➜ kubectl logs -f victoria-metrics-57d47f4587-htb88 -n kube-vm
2022-04-22T08:59:14.431Z        info    VictoriaMetrics/lib/logger/flag.go:12   build version: victoria-metrics-20220412-134346-tags-v1.76.1-0-gf8de318bf
2022-04-22T08:59:14.431Z        info    VictoriaMetrics/lib/logger/flag.go:13   command line flags
2022-04-22T08:59:14.431Z        info    VictoriaMetrics/lib/logger/flag.go:20   flag &quot;retentionPeriod&quot;=&quot;1w&quot;
2022-04-22T08:59:14.431Z        info    VictoriaMetrics/lib/logger/flag.go:20   flag &quot;storageDataPath&quot;=&quot;/var/lib/victoria-metrics-data&quot;
2022-04-22T08:59:14.431Z        info    VictoriaMetrics/app/victoria-metrics/main.go:52 starting VictoriaMetrics at &quot;:8428&quot;...
2022-04-22T08:59:14.432Z        info    VictoriaMetrics/app/vmstorage/main.go:97        opening storage at &quot;/var/lib/victoria-metrics-data&quot; with -retentionPeriod=1w
......
2022-04-22T08:59:14.449Z        info    VictoriaMetrics/app/victoria-metrics/main.go:61 started VictoriaMetrics in 0.017 seconds
2022-04-22T08:59:14.449Z        info    VictoriaMetrics/lib/httpserver/httpserver.go:91 starting http server at http://127.0.0.1:8428/
</code></pre></div>
<p>到这里我们单节点的 <code>VictoriaMetrics</code> 就部署成功了。接下来我们只需要在 Prometheus 中配置远程写入我们的 VM 即可，更改 Prometheus 配置：</p>
<div class="highlight"><pre><span></span><code># vm-prom-config2.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: kube-vm
data:
  prometheus.yaml: |
    global:
      scrape_interval: 15s
      scrape_timeout: 15s
    remote_write:    # 远程写入到远程 VM 存储
    - url: http://victoria-metrics:8428/api/v1/write
    scrape_configs:
    - job_name: &quot;nodes&quot;
      static_configs:
      - targets: [&#39;192.168.0.109:9111&#39;, &#39;192.168.0.110:9111&#39;, &#39;192.168.0.111:9111&#39;]
      relabel_configs: # 通过 relabeling 从 __address__ 中提取 IP 信息，为了后面验证 VM 是否兼容 relabeling
      - source_labels: [__address__]
        regex: &quot;(.*):(.*)&quot;
        replacement: &quot;${1}&quot;
        target_label: &#39;ip&#39;
        action: replace
</code></pre></div>
<p>重新更新 Prometheus 的配置资源对象：</p>
<div class="highlight"><pre><span></span><code>☸ ➜ kubectl apply -f vm-prom-config2.yaml
# 更新后执行 reload 操作重新加载 prometheus 配置
☸ ➜ curl -X POST &quot;http://192.168.0.111:31890/-/reload&quot;
</code></pre></div>
<p>配置生效后 Prometheus 就会开始将数据远程写入 VM 中，我们可以查看 VM 的持久化数据目录是否有数据产生来验证：</p>
<div class="highlight"><pre><span></span><code>☸ ➜ ll /data/k8s/vm/data/
total 0
drwxr-xr-x 4 root root 38 Apr 22 17:15 big
-rw-r--r-- 1 root root  0 Apr 22 16:59 flock.lock
drwxr-xr-x 4 root root 38 Apr 22 17:15 small
</code></pre></div>
<p>现在我们去直接将 Grafana 中的数据源地址修改成 VM 的地址：</p>
<p><img alt="修改数据源" src="https://picdn.youdianzhishi.com/images/1650619046819.png" /></p>
<p>修改完成后重新访问 node-exporter 的 dashboard，正常可以显示，证明 VM 是兼容的。</p>
<p><img alt="节点dashboard" src="https://picdn.youdianzhishi.com/images/1650619215909.png" /></p>
<h3 id="prometheus">替换 Prometheus<a class="headerlink" href="#prometheus" title="Permanent link">&para;</a></h3>
<p>上面我们将 Prometheus 数据远程写入到了 VM，但是 Prometheus 开启 remote write 功能后会增加其本身的资源占用，理论上其实我们也可以完全用 VM 来替换掉 Prometheus，这样就不需要远程写入了，而且本身 VM 就比 Prometheus 占用更少的资源。</p>
<p>现在我们先停掉 Prometheus 的服务：</p>
<div class="highlight"><pre><span></span><code>☸ ➜ kubectl scale deploy prometheus --replicas=0 -n kube-vm
</code></pre></div>
<p>然后将 Prometheus 的配置文件挂载到 VM 容器中，使用参数 <code>-promscrape.config</code> 来指定 Prometheus 的配置文件路径，如下所示：</p>
<div class="highlight"><pre><span></span><code># vm-single-node-deploy2.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: victoria-metrics
  namespace: kube-vm
spec:
  selector:
    matchLabels:
      app: victoria-metrics
  template:
    metadata:
      labels:
        app: victoria-metrics
    spec:
      volumes:
        - name: storage
          persistentVolumeClaim:
            claimName: victoria-metrics-data
        - name: prometheus-config
          configMap:
            name: prometheus-config
      containers:
        - name: vm
          image: victoriametrics/victoria-metrics:v1.76.1
          imagePullPolicy: IfNotPresent
          args:
            - -storageDataPath=/var/lib/victoria-metrics-data
            - -retentionPeriod=1w
            - -promscrape.config=/etc/prometheus/prometheus.yaml
          ports:
            - containerPort: 8428
              name: http
          volumeMounts:
            - mountPath: /var/lib/victoria-metrics-data
              name: storage
            - mountPath: /etc/prometheus
              name: prometheus-config
</code></pre></div>
<p>记得先将 Prometheus 配置文件中的 remote_write 模块去掉，然后重新更新 VM 即可：</p>
<div class="highlight"><pre><span></span><code>☸ ➜ kubectl apply -f vm-prom-config.yaml
☸ ➜ kubectl apply -f vm-single-node-deploy2.yaml
☸ ➜ kubectl get pods -n kube-vm -l app=victoria-metrics
NAME                                READY   STATUS    RESTARTS       AGE
victoria-metrics-8466844968-ncfnp   1/1     Running   2 (3m3s ago)   3m45s
☸ ➜ kubectl logs -f victoria-metrics-8466844968-ncfnp -n kube-vm
......
2022-04-22T10:01:59.837Z        info    VictoriaMetrics/app/victoria-metrics/main.go:61 started VictoriaMetrics in 0.022 seconds
2022-04-22T10:01:59.837Z        info    VictoriaMetrics/lib/httpserver/httpserver.go:91 starting http server at http://127.0.0.1:8428/
2022-04-22T10:01:59.837Z        info    VictoriaMetrics/lib/httpserver/httpserver.go:92 pprof handlers are exposed at http://127.0.0.1:8428/debug/pprof/
2022-04-22T10:01:59.838Z        info    VictoriaMetrics/lib/promscrape/scraper.go:103   reading Prometheus configs from &quot;/etc/prometheus/prometheus.yaml&quot;
2022-04-22T10:01:59.838Z        info    VictoriaMetrics/lib/promscrape/config.go:96     starting service discovery routines...
2022-04-22T10:01:59.839Z        info    VictoriaMetrics/lib/promscrape/config.go:102    started service discovery routines in 0.000 seconds
2022-04-22T10:01:59.840Z        info    VictoriaMetrics/lib/promscrape/scraper.go:395   static_configs: added targets: 3, removed targets: 0; total targets: 3
</code></pre></div>
<p>从 VM 日志中可以看出成功读取了 Prometheus 的配置，并抓取了 3 个指标（node-exporter）。 现在我们再去 Grafana 查看 node-exporter 的 Dashboard 是否可以正常显示。先保证数据源是 VM 的地址。</p>
<p><img alt="vm数据源" src="https://picdn.youdianzhishi.com/images/1650622089203.png" /></p>
<p>这样我们就使用 VM 替换掉了 Prometheus，我们也可以这 Grafana 的 Explore 页面去探索采集到的指标。</p>
<p><img alt="node dashboard" src="https://picdn.youdianzhishi.com/images/1650622171516.png" /></p>
<h3 id="ui">UI 界面<a class="headerlink" href="#ui" title="Permanent link">&para;</a></h3>
<p>VM 单节点版本本身自带了一个 Web UI 界面 - <a href="https://github.com/VictoriaMetrics/VictoriaMetrics/tree/master/app/vmui">vmui</a>，不过目前功能比较简单，可以直接通过 VM 的 NodePort 端口进行访问。</p>
<div class="highlight"><pre><span></span><code>☸ ➜ kubectl get svc victoria-metrics -n kube-vm
NAME               TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
victoria-metrics   NodePort   10.106.216.248   &lt;none&gt;        8428:31953/TCP   75m
</code></pre></div>
<p>我们这里可以通过 <code>http://&lt;node-ip&gt;:31953</code> 访问到 vmui：</p>
<p><img alt="vmui" src="https://picdn.youdianzhishi.com/images/1650622723400.png" /></p>
<p>可以通过 <code>/vmui</code> 这个 endpoint 访问 UI 界面：</p>
<p><img alt="vmui web ui" src="https://picdn.youdianzhishi.com/images/1650622901426.jpg" /></p>
<p>如果你想查看采集到的指标 targets，那么可以通过 <code>/targets</code> 这个 endpoint 来获取：</p>
<p><img alt="targets" src="https://picdn.youdianzhishi.com/images/1650623116166.png" /></p>
<p>这些功能基本上可以满足我们的一些需求，但是还是太过简单，如果你习惯了 Prometheus 的 UI 界面，那么我们可以使用 <code>promxy</code> 来代替 <code>vmui</code>，而且 <code>promxy</code> 还可以进行多个 VM 单节点的数据聚合，以及 targets 查看等，对应的资源清单文件如下所示：</p>
<div class="highlight"><pre><span></span><code># vm-promxy.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: promxy-config
  namespace: kube-vm
data:
  config.yaml: |
    promxy:
      server_groups:
      - static_configs:
        - targets: [victoria-metrics:8428]  # 指定vm地址，有多个则往后追加即可
        path_prefix: /prometheus  # 配置前缀
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: promxy
  namespace: kube-vm
spec:
  selector:
    matchLabels:
      app: promxy
  template:
    metadata:
      labels:
        app: promxy
    spec:
      containers:
        - args:
            - &quot;--config=/etc/promxy/config.yaml&quot;
            - &quot;--web.enable-lifecycle&quot;
            - &quot;--log-level=trace&quot;
          env:
            - name: ROLE
              value: &quot;1&quot;
          command:
            - &quot;/bin/promxy&quot;
          image: quay.io/jacksontj/promxy
          imagePullPolicy: Always
          name: promxy
          ports:
            - containerPort: 8082
              name: web
          volumeMounts:
            - mountPath: &quot;/etc/promxy/&quot;
              name: promxy-config
              readOnly: true
        - args: # container to reload configs on configmap change
            - &quot;--volume-dir=/etc/promxy&quot;
            - &quot;--webhook-url=http://localhost:8082/-/reload&quot;
          image: jimmidyson/configmap-reload:v0.1
          name: promxy-server-configmap-reload
          volumeMounts:
            - mountPath: &quot;/etc/promxy/&quot;
              name: promxy-config
              readOnly: true
      volumes:
        - configMap:
            name: promxy-config
          name: promxy-config
---
apiVersion: v1
kind: Service
metadata:
  name: promxy
  namespace: kube-vm
spec:
  type: NodePort
  ports:
    - port: 8082
  selector:
    app: promxy
</code></pre></div>
<p>直接应用上面的资源对象即可：</p>
<div class="highlight"><pre><span></span><code>☸ ➜ kubectl apply -f vm-promxy.yaml
☸ ➜ kubectl get pods -n kube-vm -l app=promxy
NAME                      READY   STATUS    RESTARTS   AGE
promxy-5f7dfdbc64-l4kjq   2/2     Running   0          6m45s
☸ ➜ kubectl get svc promxy -n kube-vm
NAME               TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
promxy             NodePort   10.110.19.254    &lt;none&gt;        8082:30618/TCP   6m12s
</code></pre></div>
<p>访问 Promxy 的页面效果和 Prometheus 自带的 Web UI 基本一致的。</p>
<p><img alt="promxy" src="https://picdn.youdianzhishi.com/images/1650624844155.png" /></p>
<p>这里面我们简单介绍了单机版的 <code>victoriametrics</code> 的基本使用。</p>
<h2 id="_3">集群版<a class="headerlink" href="#_3" title="Permanent link">&para;</a></h2>
<p>对于低于每秒一百万个数据点的摄取率，建议使用单节点版本而不是集群版本。单节点版本可根据 CPU 内核、RAM 和可用存储空间的数量进行扩展。单节点版本比集群版本更容易配置和操作，所以在使用集群版本之前要三思而后行。上面我们介绍了 VM 的单节点版本的基本使用，接下来我们来介绍下如何使用集群版。</p>
<p>集群版主要特点：</p>
<ul>
<li>支持单节点版本的所有功能。</li>
<li>性能和容量水平扩展。</li>
<li>支持时间序列数据的多个独立命名空间（多租户）。</li>
<li>支持多副本。</li>
</ul>
<p><strong>组件服务</strong></p>
<p>前面我们了解了 VM 的基本架构，对于集群模式下主要包含以下几个服务：</p>
<ul>
<li><code>vmstorage</code>：存储原始数据并返回指定标签过滤器在给定时间范围内的查询数据，当 <code>-storageDataPath</code> 指向的目录包含的可用空间少于 <code>-storage.minFreeDiskSpaceBytes</code> 时，<code>vmstorage</code> 节点会自动切换到<strong>只读模式</strong> ，<code>vminsert</code> 节点也会停止向此类节点发送数据并开始将数据重新路由到剩余的 <code>vmstorage</code> 节点。</li>
<li><code>vminsert</code>：接受摄取的数据并根据指标名称及其所有标签的一致性哈希将其分散存储到 <code>vmstorage</code> 节点。</li>
<li><code>vmselect</code>：通过从所有配置的 <code>vmstorage</code> 节点获取所需数据来执行查询。</li>
</ul>
<p>每个服务都可以进行独立扩展，<code>vmstorage</code> 节点之间互不了解、互不通信，并且不共享任何数据。这样可以增加集群的可用性，并且简化了集群的维护和扩展。</p>
<p>最小集群必须包含以下节点：</p>
<ul>
<li>带有 <code>-retentionPeriod</code> 和 <code>-storageDataPath</code> 参数的单 <code>vmstorage</code> 节点</li>
<li>带有 <code>-storageNode=&lt;vmstorage_host&gt;</code> 的单 <code>vminsert</code> 节点</li>
<li>带有 <code>-storageNode=&lt;vmstorage_host&gt;</code> 的单 <code>vmselect</code> 节点</li>
</ul>
<p>但是我们建议为每个服务组件运行至少两个节点以实现高可用性，这样当单个节点暂时不可用时，集群会继续工作，而且其余节点还可以处理增加的工作负载。如果你的集群规模较大，那么可以运行多个小型的 <code>vmstorage</code> 节点，因为这样可以在某些 <code>vmstorage</code> 节点暂时不可用时减少剩余 <code>vmstorage</code> 节点上的工作负载增加。</p>
<p>各个服务除了可以通过参数标志进行配置之外，也可以通过环境变量的方式进行配置：</p>
<ul>
<li><code>-envflag.enable</code> 标志必须设置</li>
<li>每个标志中的 <code>.</code> 必须替换为 <code>_</code>，例如 <code>-insert.maxQueueDuration &lt;duration&gt;</code> 可以转换为 <code>insert_maxQueueDuration=&lt;duration&gt;</code></li>
<li>对于重复的标志，可以使用另一种语法，通过使用 <code>,</code> 作为分隔符将不同的值连接成一个，例如 <code>-storageNode &lt;nodeA&gt; -storageNode &lt;nodeB&gt;</code> 将转换为 <code>-storageNode=&lt;nodeA&gt;,&lt;nodeB&gt;</code></li>
<li>可以使用 <code>-envflag.prefix</code> 为环境变量设置前缀，例如设置了 <code>-envflag.prefix=VM*</code>，则环境变量参数必须以 <code>VM*</code> 开头</li>
</ul>
<p><strong>多租户</strong></p>
<p>此外 VM 集群也支持多个独立的租户（也叫命名空间），租户由 <code>accountID</code> 或 <code>accountID:projectID</code> 来标识，它们被放在请求的 urls 中。</p>
<ul>
<li>每个 <code>accountID</code> 和 <code>projectID</code> 都由一个 [0 .. 2^32] 范围内的任意 32 位整数标识，如果缺少 <code>projectID</code>，则自动将其分配为 0。有关租户的其他信息，例如身份验证令牌、租户名称、限额、计费等，将存储在一个单独的关系型数据库中。此数据库必须由位于 VictoriaMetrics 集群前面的单独服务管理，例如 <code>vmauth</code> 或 <code>vmgateway</code>。</li>
<li>当第一个数据点写入指定租户时，租户被自动创建。</li>
<li>所有租户的数据均匀分布在可用的 <code>vmstorage</code> 节点中，当不同租户有不同的数据量和不同的查询负载时，这保证了 <code>vmstorage</code> 节点之间的均匀负载。</li>
<li>数据库性能和资源使用不依赖于租户的数量，它主要取决于所有租户中活跃时间序列的总数。如果一个时间序列在过去一小时内至少收到一个样本，或者在过去一小时内被查询，则认为时间序列是活跃的。</li>
<li>VictoriaMetrics 不支持在单个请求中查询多个租户。</li>
</ul>
<p><strong>集群大小调整和可扩展性</strong></p>
<p>VM 集群的性能和容量可以通过两种方式进行扩展：</p>
<ul>
<li>通过向集群中的现有节点添加更多资源（CPU、RAM、磁盘 IO、磁盘空间、网络带宽），也叫垂直可扩展性。</li>
<li>通过向集群添加更多节点，又叫水平扩展性。</li>
</ul>
<p>对于集群扩展有一些通用的建议：</p>
<ul>
<li>向现有 <code>vmselect</code> 节点添加更多 CPU 和内存，可以提高复杂查询的性能，这些查询可以处理大量的时间序列和大量的原始样本。</li>
<li>添加更多 <code>vmstorage</code> 节点可以增加集群可以处理的活跃时间序列的数量，这也提高了对高流失率(<code>churn rate</code>)的时间序列的查询性能。集群稳定性也会随着 <code>vmstorage</code> 节点数量的增加而提高，当一些 <code>vmstorage</code> 节点不可用时，活跃的 <code>vmstorage</code> 节点需要处理较低的额外工作负载。</li>
<li>向现有 <code>vmstorage</code> 节点添加更多 CPU 和内存，可以增加集群可以处理的活跃时间序列的数量。与向现有 <code>vmstorage</code> 节点添加更多 CPU 和内存相比，最好添加更多 <code>vmstorage</code> 节点，因为更多的 <code>vmstorage</code> 节点可以提高集群稳定性，并提高对高流失率的时间序列的查询性能。</li>
<li>添加更多的 <code>vminsert</code> 节点会提高数据摄取的最大速度，因为摄取的数据可以在更多的 <code>vminsert</code> 节点之间进行拆分。</li>
<li>添加更多的 <code>vmselect</code> 节点可以提高查询的最大速度，因为传入的并发请求可能会在更多的 <code>vmselect</code> 节点之间进行拆分。</li>
</ul>
<p><strong>集群可用性</strong></p>
<ul>
<li>HTTP 负载均衡器需要停止将请求路由到不可用的 <code>vminsert</code> 和 <code>vmselect</code> 节点。</li>
<li>如果至少存在一个 <code>vmstorage</code> 节点，则集群仍然可用：</li>
<li><code>vminsert</code> 将传入数据从不可用的 <code>vmstorage</code> 节点重新路由到健康的 <code>vmstorage</code> 节点</li>
<li>如果至少有一个 <code>vmstorage</code> 节点可用，则 <code>vmselect</code> 会继续提供部分响应。如果优先考虑可用性的一致性，则将 <code>-search.denyPartialResponse</code> 标志传递给 <code>vmselect</code> 或将请求中的 <code>deny_partial_response=1</code> 查询参数传递给 <code>vmselect</code>。</li>
</ul>
<p><strong>重复数据删除</strong></p>
<p>如果 <code>-dedup.minScrapeInterval</code> 命令行标志设置为大于 0 的时间，VictoriaMetrics 会去除重复数据点。例如，<code>-dedup.minScrapeInterval=60s</code> 将对同一时间序列上的数据点进行重复数据删除，如果它们位于同一离散的 60 秒存储桶内，最早的数据点将被保留。在时间戳相等的情况下，将保留任意数据点。</p>
<p><code>-dedup.minScrapeInterval</code> 的推荐值是等于 Prometheus 配置中的 <code>scrape_interval</code> 的值，建议在所有抓取目标中使用一个 <code>scrape_interval</code> 配置。</p>
<p>如果 HA 中多个相同配置的 <code>vmagent</code> 或 Prometheus 实例将数据写入同一个 VictoriaMetrics 实例，则重复数据删除会减少磁盘空间使用。这些 <code>vmagent</code> 或 Prometheus 实例在其配置中必须具有相同的 <code>external_labels</code> 部分，因此它们将数据写入相同的时间序列。</p>
<p><strong>容量规划</strong></p>
<p>根据我们的案例研究，与竞争解决方案（Prometheus、Thanos、Cortex、TimescaleDB、InfluxDB、QuestDB、M3DB）相比，VictoriaMetrics 在生产工作负载上使用的 CPU、内存和存储空间更少。</p>
<p>每种节点类型 - <code>vminsert</code>、<code>vmselect</code> 和 <code>vmstorage</code> 都可以在最合适的硬件上运行。集群容量随着可用资源的增加而线性扩展。每个节点类型所需的 CPU 和内存数量很大程度上取决于工作负载 - 活跃时间序列的数量、序列流失率、查询类型、查询 qps 等。建议为你的生产工作负载部署一个测试的 VictoriaMetrics 集群，并反复调整每个节点的资源和每个节点类型的节点数量，直到集群变得稳定。同样也建议为集群设置监控，有助于确定集群设置中的瓶颈问题。</p>
<p>指定保留所需的存储空间（可以通过 <code>vmstorage</code> 中的 <code>-retentionPeriod</code> 命令行标志设置）可以从测试运行中的磁盘空间使用情况推断出来。例如，如果在生产工作负载上运行一天后的存储空间使用量为 10GB，那么对于 <code>-retentionPeriod=100d</code>（100 天保留期）来说，它至少需要 <code>10GB*100=1TB</code> 的磁盘空间。可以使用 VictoriaMetrics 集群的<a href="https://grafana.com/grafana/dashboards/11176">官方 Grafana 仪表板</a>监控存储空间使用情况。</p>
<p>建议留出以下数量的备用资源。</p>
<ul>
<li>所有节点类型中 50% 的空闲内存，以减少工作负载临时激增时因为 OOM 崩溃的可能性。</li>
<li>所有节点类型中 50% 的空闲 CPU，以减少工作负载临时高峰期间的慢速概率。</li>
<li><code>vmstorage</code> 节点上 <code>-storageDataPath</code> 命令行标志指向的目录中至少有 <strong>30%</strong> 的可用存储空间。</li>
</ul>
<p>VictoriaMetrics 集群的一些容量规划技巧：</p>
<ul>
<li>副本集将集群所需的资源量最多增加 N 倍，其中 N 是复制因子。</li>
<li>可以通过添加更多 <code>vmstorage</code> 节点和/或通过增加每个 <code>vmstorage</code> 节点的内存和 CPU 资源来增加活跃时间序列的集群容量。</li>
<li>可以通过增加 <code>vmstorage</code> 节点的数量和/或通过增加每个 <code>vmselect</code> 节点的内存和 CPU 资源来减少查询延迟。</li>
<li>所有 <code>vminsert</code> 节点所需的 CPU 内核总数可以通过摄取率计算：<code>CPUs = ingestion_rate / 100K</code>。</li>
<li><code>vminsert</code> 节点上的 <code>-rpc.disableCompression</code> 命令行标志可以增加摄取容量，但代价是 <code>vminsert</code> 和 <code>vmstorage</code> 之间的网络带宽使用率会更高。</li>
</ul>
<p><strong>复制和数据安全</strong></p>
<p>默认情况下，VictoriaMetrics 的数据复制依赖 <code>-storageDataPath</code> 指向的底层存储来完成。</p>
<p>但是我们也可以手动通过将 <code>-replicationFactor=N</code> 命令参数传递给 <code>vminsert</code> 来启用复制，这保证了如果多达 <code>N-1</code> 个 <code>vmstorage</code> 节点不可用，所有数据仍可用于查询。集群必须至少包含 <code>2*N-1</code> 个 <code>vmstorage</code> 节点，其中 <code>N</code> 是复制因子，以便在 <code>N-1</code> 个存储节点丢失时为新摄取的数据维持指定的复制因子。</p>
<p>例如，当 <code>-replicationFactor=3</code> 传递给 <code>vminsert</code> 时，它将所有摄取的数据复制到 3 个不同的 <code>vmstorage</code> 节点，因此最多可以丢失 2 个 <code>vmstorage</code> 节点而不会丢失数据。<code>vmstorage</code> 节点的最小数量应该等于 <code>2*3-1 = 5</code>，因此当 2 个 <code>vmstorage</code> 节点丢失时，剩余的 3 个 <code>vmstorage</code> 节点可以为新摄取的数据提供服务。</p>
<p>启用复制后，必须将 <code>-dedup.minScrapeInterval=1ms</code> 命令行标志传递给 <code>vmselect</code> 节点，当多达 <code>N-1</code> 个 <code>vmstorage</code> 节点响应缓慢和/或暂时不可用时，可以将可选的 <code>-replicationFactor=N</code> 参数传递给 <code>vmselect</code> 以提高查询性能，因为 <code>vmselect</code> 不等待来自多达 <code>N-1</code> 个 <code>vmstorage</code> 节点的响应。有时，<code>vmselect</code> 节点上的 <code>-replicationFactor</code> 可能会导致部分响应。<code>-dedup.minScrapeInterval=1ms</code> 在查询期间对复制的数据进行重复数据删除，如果重复数据从配置相同的 <code>vmagent</code> 实例或 Prometheus 实例推送到 VictoriaMetrics，则必须根据重复数据删除文档将 <code>-dedup.minScrapeInterval</code> 设置为更大的值。 请注意，复制不会从灾难中保存，因此建议执行定期备份。另外 复制会增加资源使用率 - CPU、内存、磁盘空间、网络带宽 - 最多 <code>-replicationFactor</code> 倍。所以可以将复制转移 <code>-storageDataPath</code> 指向的底层存储来做保证，例如 Google Compute Engine 永久磁盘，该磁盘可以防止数据丢失和数据损坏，它还提供始终如一的高性能，并且可以在不停机的情况下调整大小。对于大多数用例来说，基于 HDD 的永久性磁盘应该足够了。</p>
<p><strong>备份</strong></p>
<p>建议从即时快照执行定期备份，以防止意外数据删除等错误。必须为每个 <code>vmstorage</code> 节点执行以下步骤来创建备份：</p>
<p>通过导航到/snapshot/create HTTP handler 来创建一个即时快照。它将创建快照并返回其名称。</p>
<ul>
<li>可以通过访问 <code>/snapshot/create</code> 这个 HTTP handler 来创建即时快照，它将创建快照并返回其名称。</li>
<li>使用 <code>vmbackup</code> 组件从 <code>&lt;-storageDataPath&gt;/snapshots/&lt;snapshot_name&gt;</code> 文件夹归档创建的快照。归档过程不会干扰 <code>vmstorage</code> 工作，因此可以在任何合适的时间执行。</li>
<li>通过 <code>/snapshot/delete?snapshot=&lt;snapshot_name&gt;</code> 或 <code>/snapshot/delete_all</code> 删除未使用的快照，以释放占用的存储空间。</li>
<li>无需在所有 <code>vmstorage</code> 节点之间同步备份。</li>
</ul>
<p>从备份恢复：</p>
<ul>
<li>使用 <code>kill -INT</code> 停止 <code>vmstorage</code> 节点。</li>
<li>使用 <code>vmrestore</code> 组件将备份中的数据还原到 <code>-storageDataPath</code> 目录。</li>
<li>启动 <code>vmstorage</code> 节点。</li>
</ul>
<p>在了解了 VM 集群的一些配置细节后，接下来我们就来开始部署 VM 集群。</p>
<p><strong>Helm</strong></p>
<p>如果你已经对 VM 组件非常了解了，那么推荐使用 Helm Chart 的方式进行一键安装。</p>
<div class="highlight"><pre><span></span><code>☸ ➜ helm repo add vm https://victoriametrics.github.io/helm-charts/
☸ ➜ helm repo update
# 导出默认的 values 值到 values.yaml 文件中
☸ ➜ helm show values vm/victoria-metrics-cluster &gt; values.yaml
# 根据自己的需求修改 values.yaml 文件配置
# 执行下面的命令进行一键安装
☸ ➜ helm install victoria-metrics vm/victoria-metrics-cluster -f values.yaml -n NAMESPACE
# 获取 vm 运行的 pods 列表
☸ ➜ kubectl get pods -A | grep &#39;victoria-metrics&#39;
</code></pre></div>
<p>我们这里选择手动方式进行部署，之所以选择手动部署的方式是为了能够了解各个组件的更多细节。</p>
<p><strong>手动安装</strong></p>
<p>由于 vmstorage 组件是有状态的，这里我们先使用 StatefulSet 进行部署，由于该组件也是可以进行扩展的，这里我们首先部署两个副本，对应的资源清单如下所示：</p>
<div class="highlight"><pre><span></span><code># cluster-vmstorage.yaml
apiVersion: v1
kind: Service
metadata:
  name: cluster-vmstorage
  namespace: kube-vm
  labels:
    app: vmstorage
spec:
  clusterIP: None
  ports:
    - port: 8482
      targetPort: http
      name: http
    - port: 8401
      targetPort: vmselect
      name: vmselect
    - port: 8400
      targetPort: vminsert
      name: vminsert
  selector:
    app: vmstorage
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: vmstorage
  namespace: kube-vm
  labels:
    app: vmstorage
spec:
  serviceName: cluster-vmstorage
  selector:
    matchLabels:
      app: vmstorage
  replicas: 2
  podManagementPolicy: OrderedReady
  template:
    metadata:
      labels:
        app: vmstorage
    spec:
      containers:
        - name: vmstorage
          image: &quot;victoriametrics/vmstorage:v1.77.0-cluster&quot;
          imagePullPolicy: &quot;IfNotPresent&quot;
          args:
            - &quot;--retentionPeriod=1&quot;
            - &quot;--storageDataPath=/storage&quot;
            - --envflag.enable=true
            - --envflag.prefix=VM_
            - --loggerFormat=json
          ports:
            - name: http
              containerPort: 8482
            - name: vminsert
              containerPort: 8400
            - name: vmselect
              containerPort: 8401
          livenessProbe:
            failureThreshold: 10
            initialDelaySeconds: 30
            periodSeconds: 30
            tcpSocket:
              port: http
            timeoutSeconds: 5
          readinessProbe:
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 15
            timeoutSeconds: 5
            httpGet:
              path: /health
              port: http
          volumeMounts:
            - name: storage
              mountPath: /storage
  volumeClaimTemplates:
    - metadata:
        name: storage
      spec:
        storageClassName: longhorn
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: &quot;2Gi&quot;
</code></pre></div>
<p>首先需要创建一个 Headless 的 Service，因为后面的组件需要访问到每一个具体的 Pod，在 vmstorage 启动参数中通过 <code>--retentionPeriod</code> 参数指定指标数据保留时长，1 表示一个月，这也是默认的时长，然后通过 <code>--storageDataPath</code> 参数指定了数据存储路径，记得要将该目录进行持久化。</p>
<p>同样直接应用该资源即可：</p>
<div class="highlight"><pre><span></span><code>☸ ➜ kubectl apply -f cluster-vmstorage.yaml
☸ ➜ kubectl get pods -n kube-vm -l app=vmstorage
NAME          READY   STATUS    RESTARTS   AGE
vmstorage-0   1/1     Running   0          5m40s
vmstorage-1   1/1     Running   0          3m31s
☸ ➜ kubectl get svc -n kube-vm -l app=vmstorage
NAME                TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                      AGE
cluster-vmstorage   ClusterIP   None         &lt;none&gt;        8482/TCP,8401/TCP,8400/TCP   5m46s
</code></pre></div>
<p>接着可以部署 vmselect 组件，由于该组件是无状态的，我们可以直接使用 Deployment 来进行管理，对应的资源清单文件如下所示：</p>
<div class="highlight"><pre><span></span><code># cluster-vmselect.yaml
apiVersion: v1
kind: Service
metadata:
  name: vmselect
  namespace: kube-vm
  labels:
    app: vmselect
spec:
  ports:
    - name: http
      port: 8481
      targetPort: http
  selector:
    app: vmselect
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vmselect
  namespace: kube-vm
  labels:
    app: vmselect
spec:
  selector:
    matchLabels:
      app: vmselect
  template:
    metadata:
      labels:
        app: vmselect
    spec:
      containers:
        - name: vmselect
          image: &quot;victoriametrics/vmselect:v1.77.0-cluster&quot;
          imagePullPolicy: &quot;IfNotPresent&quot;
          args:
            - &quot;--cacheDataPath=/cache&quot;
            - --storageNode=vmstorage-0.cluster-vmstorage.kube-vm.svc.cluster.local:8401
            - --storageNode=vmstorage-1.cluster-vmstorage.kube-vm.svc.cluster.local:8401
            - --envflag.enable=true
            - --envflag.prefix=VM_
            - --loggerFormat=json
          ports:
            - name: http
              containerPort: 8481
          readinessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 5
            periodSeconds: 15
            timeoutSeconds: 5
            failureThreshold: 3
          livenessProbe:
            tcpSocket:
              port: http
            initialDelaySeconds: 5
            periodSeconds: 15
            timeoutSeconds: 5
            failureThreshold: 3
          volumeMounts:
            - mountPath: /cache
              name: cache-volume
      volumes:
        - name: cache-volume
          emptyDir: {}
</code></pre></div>
<p>其中最重要的部分是通过 <code>--storageNode</code> 参数指定所有的 vmstorage 节点地址，上面我们使用的 StatefulSet 部署的，所以可以直接使用 FQDN 的形式进行访问。直接应用上面的对象：</p>
<div class="highlight"><pre><span></span><code>☸ ➜ kubectl apply -f cluster-vmselect.yaml
☸ ➜ kubectl get pods -n kube-vm -l app=vmselect
NAME                       READY   STATUS    RESTARTS   AGE
vmselect-bcb54965f-5rkml   1/1     Running   0          2m4s
☸ ➜ kubectl get svc -n kube-vm -l app=vmselect
NAME       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
vmselect   ClusterIP   10.107.227.214   &lt;none&gt;        8481/TCP   2m17s
</code></pre></div>
<p>如果要进行查询，那么我们可以直接对外暴露 vmselect 这个 Service 服务即可，修改 Grafana 数据源地址为 <code>http://&lt;select-service&gt;/select/0/prometheus/</code>。</p>
<p><img alt="vmselect数据源" src="https://picdn.youdianzhishi.com/images/1651826079461.png" /></p>
<p>接着就需要部署用来接收指标数据插入的 vminsert 组件，同样该组件是无状态的，其中最重要的也是需要通过 <code>--storageNode</code> 参数指定所有的 vmstorage 节点：</p>
<div class="highlight"><pre><span></span><code># cluster-vminsert.yaml
apiVersion: v1
kind: Service
metadata:
  name: vminsert
  namespace: kube-vm
  labels:
    app: vminsert
spec:
  ports:
    - name: http
      port: 8480
      targetPort: http
  selector:
    app: vminsert
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vminsert
  namespace: kube-vm
  labels:
    app: vminsert
spec:
  selector:
    matchLabels:
      app: vminsert
  template:
    metadata:
      labels:
        app: vminsert
    spec:
      containers:
        - name: vminsert
          image: &quot;victoriametrics/vminsert:v1.77.0-cluster&quot;
          imagePullPolicy: &quot;IfNotPresent&quot;
          args:
            - --storageNode=vmstorage-0.cluster-vmstorage.kube-vm.svc.cluster.local:8400
            - --storageNode=vmstorage-1.cluster-vmstorage.kube-vm.svc.cluster.local:8400
            - --envflag.enable=true
            - --envflag.prefix=VM_
            - --loggerFormat=json
          ports:
            - name: http
              containerPort: 8480
          readinessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 5
            periodSeconds: 15
            timeoutSeconds: 5
            failureThreshold: 3
          livenessProbe:
            tcpSocket:
              port: http
            initialDelaySeconds: 5
            periodSeconds: 15
            timeoutSeconds: 5
            failureThreshold: 3
</code></pre></div>
<p>由于本身是无状态的，所以可以根据需要增加副本数量，也可以配置 HPA 进行自动扩缩容。直接应用上面的资源清单：</p>
<div class="highlight"><pre><span></span><code>☸ ➜ kubectl apply -f cluster-vminsert.yaml
☸ ➜ kubectl get pods -n kube-vm -l app=vminsert
NAME                        READY   STATUS    RESTARTS   AGE
vminsert-66c88cd497-l64ps   1/1     Running   0          2m27s
☸ ➜ kubectl get svc -n kube-vm -l app=vminsert
NAME       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
vminsert   ClusterIP   10.96.125.134   &lt;none&gt;        8480/TCP   70s
</code></pre></div>
<p>集群模式的相关组件部署完成后，同样我们可以先去配置前面的 Prometheus，将其数据远程写入到 VM 中来，修改 <code>remote_write</code> 的地址为 <code>http://vminsert:8480/insert/0/prometheus/</code>，注意和单节点模式的 API 路径不一样，如下所示：</p>
<div class="highlight"><pre><span></span><code># vm-prom-config3.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: kube-vm
data:
  prometheus.yaml: |
    global:
      scrape_interval: 15s
      scrape_timeout: 15s
    remote_write:    # 写入到远程 VM 存储，url 是远程写入接口地址
    - url: http://vminsert:8480/insert/0/prometheus/
      # queue_config:    # 如果 Prometheus 抓取指标很大，可以加调整 queue，但是会提高内存占用
      #   max_samples_per_send: 10000  # 每次发送的最大样本数
      #   capacity: 20000
      #   max_shards: 30   # 最大分片数，即并发量。
    scrape_configs:
    - job_name: &quot;nodes&quot;
      static_configs:
      - targets: [&#39;192.168.0.109:9111&#39;, &#39;192.168.0.110:9111&#39;, &#39;192.168.0.111:9111&#39;]
      relabel_configs: # 通过 relabeling 从 __address__ 中提取 IP 信息，为了后面验证 VM 是否兼容 relabeling
      - source_labels: [__address__]
        regex: &quot;(.*):(.*)&quot;
        replacement: &quot;${1}&quot;
        target_label: &#39;ip&#39;
        action: replace
</code></pre></div>
<p>更新 Prometheus 配置，然后启动 Prometheus，前面的单机模式的 VM 可以先停掉：</p>
<div class="highlight"><pre><span></span><code>☸ ➜ kubectl apply -f vm-prom-config3.yaml
☸ ➜ kubectl scale deploy victoria-metrics --replicas=0 -n kube-vm
☸ ➜ kubectl scale deploy prometheus --replicas=1 -n kube-vm
</code></pre></div>
<p>配置成功后正常数据就可以开始写入到 vmstorage 了，查看 vmstorage 日志可以看到成功创建了 partition，证明现在已经在开始接收数据了：</p>
<div class="highlight"><pre><span></span><code>☸ ➜ kubectl logs -f vmstorage-0 -n kube-vm
......
{&quot;ts&quot;:&quot;2022-05-06T08:35:15.786Z&quot;,&quot;level&quot;:&quot;info&quot;,&quot;caller&quot;:&quot;VictoriaMetrics/lib/storage/partition.go:206&quot;,&quot;msg&quot;:&quot;creating a partition \&quot;2022_05\&quot; with smallPartsPath=\&quot;/storage/data/small/2022_05\&quot;, bigPartsPath=\&quot;/storage/data/big/2022_05\&quot;&quot;}
{&quot;ts&quot;:&quot;2022-05-06T08:35:15.802Z&quot;,&quot;level&quot;:&quot;info&quot;,&quot;caller&quot;:&quot;VictoriaMetrics/lib/storage/partition.go:222&quot;,&quot;msg&quot;:&quot;partition \&quot;2022_05\&quot; has been created&quot;}
</code></pre></div>
<p>然后可以去 Grafana 重新查看 Dashboard 是否正常：</p>
<p><img alt="node exporter" src="https://picdn.youdianzhishi.com/images/1651826519780.jpg" /></p>
<p>如果现在需要新增 <code>vmstorage</code> 节点，那么需要按照下面的步骤进行操作：</p>
<ul>
<li>使用与集群中现有节点相同的 <code>-retentionPeriod</code> 配置启动新的 <code>vmstorage</code> 节点。</li>
<li>逐步重新启动所有的 <code>vmselect</code> 节点，添加新的 <code>-storageNode</code> 参数包含 <code>&lt;new_vmstorage_host&gt;</code>。</li>
<li>逐步重新启动所有的 <code>vminsert</code> 节点，添加新的 <code>-storageNode</code> 参数包含 <code>&lt;new_vmstorage_host&gt;</code>。</li>
</ul>
<h2 id="vmagent">vmagent<a class="headerlink" href="#vmagent" title="Permanent link">&para;</a></h2>
<p>vmagent 可以帮助我们从各种来源收集指标并将它们存储这 VM 或者任何其他支持 remote write 协议的 Prometheus 兼容的存储系统中。vmagent 相比于 Prometheus 抓取指标来说具有更多的灵活性，比如除了拉取（pull）指标还可以推送（push）指标，此外还有很多其他特性：</p>
<ul>
<li>可以替换 prometheus 的 scraping target</li>
<li>支持从 Kafka 读写数据</li>
<li>支持基于 prometheus relabeling 的模式添加、移除、修改 labels，可以在数据发送到远端存储之前进行数据的过滤</li>
<li>支持多种数据协议，influx line 协议，graphite 文本协议，opentsdb 协议，prometheus remote write 协议，json lines 协议，csv 数据等</li>
<li>支持收集数据的同时，并复制到多种远端存储系统</li>
<li>支持不可靠远端存储，如果远程存储不可用，收集的指标会在 <code>-remoteWrite.tmpDataPath</code> 缓冲，一旦与远程存储的连接被修复，缓冲的指标就会被发送到远程存储，缓冲区的最大磁盘用量可以用 <code>-remoteWrite.maxDiskUsagePerURL</code> 来限制。</li>
<li>相比 prometheus 使用更少的内存、cpu、磁盘 io 以及网络带宽</li>
<li>当需要抓取大量目标时，抓取目标可以分散到多个 vmagent 实例中</li>
<li>可以通过在抓取时间和将其发送到远程存储系统之前限制唯一时间序列的数量来处理高基数和高流失率问题</li>
<li>可以从多个文件中加载 scrape 配置</li>
</ul>
<p><img alt="vmagent" src="https://picdn.youdianzhishi.com/images/1650529595671.jpg" /></p>
<p>接下来我们以抓取 Kubernetes 集群指标为例说明如何使用 vmagent，我们这里使用自动发现的方式来进行配置。vmagent 是兼容 prometheus 中的 <code>kubernetes_sd_configs</code> 配置的，所以我们同样可以使用。</p>
<p>要让 vmagent 自动发现监控的资源对象，需要访问 APIServer 获取资源对象，所以首先需要配置 rbac 权限，创建如下所示的资源清单。</p>
<div class="highlight"><pre><span></span><code># vmagent-rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: vmagent
  namespace: kube-vm
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: vmagent
rules:
  - apiGroups: [&quot;&quot;, &quot;networking.k8s.io&quot;, &quot;extensions&quot;]
    resources:
      - nodes
      - nodes/metrics
      - services
      - endpoints
      - endpointslices
      - pods
      - app
      - ingresses
    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]
  - apiGroups: [&quot;&quot;]
    resources:
      - namespaces
      - configmaps
    verbs: [&quot;get&quot;]
  - nonResourceURLs: [&quot;/metrics&quot;, &quot;/metrics/resources&quot;]
    verbs: [&quot;get&quot;]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: vmagent
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: vmagent
subjects:
  - kind: ServiceAccount
    name: vmagent
    namespace: kube-vm
</code></pre></div>
<p>然后添加 vmagent 配置，我们先只配置自动发现 Kubernetes 节点的任务，创建如下所示的 ConfigMap 对象：</p>
<div class="highlight"><pre><span></span><code># vmagent-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: vmagent-config
  namespace: kube-vm
data:
  scrape.yml: |
    global:
      scrape_interval: 15s
      scrape_timeout: 15s

    scrape_configs:
    - job_name: nodes
      kubernetes_sd_configs:
        - role: node
      relabel_configs:
      - source_labels: [__address__]
        regex: &quot;(.*):10250&quot;
        replacement: &quot;${1}:9111&quot;
        target_label: __address__
        action: replace
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
</code></pre></div>
<p>这里我们通过自动发现 Kubernetes 节点获取节点监控指标，需要注意 <code>node</code> 这种 role 的自动发现默认获取的是节点的 <code>10250</code> 端口，这里我们需要通过 <code>relabel</code> 将其 <code>replace</code> 为 <code>9111</code>。</p>
<p>然后添加 vmagent 部署资源清单，如下所示：</p>
<div class="highlight"><pre><span></span><code># vmagent-deploy.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: vmagent-pvc
  namespace: kube-vm
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: nfs-client
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vmagent
  namespace: kube-vm
  labels:
    app: vmagent
spec:
  selector:
    matchLabels:
      app: vmagent
  template:
    metadata:
      labels:
        app: vmagent
    spec:
      serviceAccountName: vmagent
      containers:
        - name: agent
          image: &quot;victoriametrics/vmagent:v1.77.0&quot;
          imagePullPolicy: IfNotPresent
          args:
            - -promscrape.config=/config/scrape.yml
            - -remoteWrite.tmpDataPath=/tmpData
            - -remoteWrite.url=http://vminsert:8480/insert/0/prometheus
            - -envflag.enable=true
            - -envflag.prefix=VM_
            - -loggerFormat=json
          ports:
            - name: http
              containerPort: 8429
          volumeMounts:
            - name: tmpdata
              mountPath: /tmpData
            - name: config
              mountPath: /config
      volumes:
        - name: tmpdata
          persistentVolumeClaim:
            claimName: vmagent-pvc
        - name: config
          configMap:
            name: vmagent-config
</code></pre></div>
<p>我们将 vmagent 配置通过 ConfigMap 挂载到容器 <code>/config/scrape.yml</code>，另外通过 <code>-remoteWrite.url=http://vminsert:8480/insert/0/prometheus</code> 指定远程写入的地址，这里我们写入前面的 vminsert 服务，另外有一个参数 <code>-remoteWrite.tmpDataPath</code>，该路径会在远程存储不可用的时候用来缓存收集的指标，当远程存储修复后，缓存的指标就会被正常发送到远程写入，所以最好持久化该目录。</p>
<p>单个 vmagent 实例可以抓取数万个抓取目标，但是有时由于 CPU、网络、内存等方面的限制，这还不够。在这种情况下，抓取目标可以在多个 vmagent 实例之间进行拆分。集群中的每个 vmagent 实例必须使用具有不同 <code>-promscrape.cluster.memberNum</code> 值的相同 <code>-promscrape.config</code> 配置文件，该参数值必须在 <code>0 ... N-1</code> 范围内，其中 <code>N</code> 是集群中 vmagent 实例的数量。集群中 vmagent 实例的数量必须传递给 <code>-promscrape.cluster.membersCount</code> 命令行标志。例如，以下命令可以在两个 vmagent 实例的集群中传播抓取目标：</p>
<div class="highlight"><pre><span></span><code>vmagent -promscrape.cluster.membersCount=2 -promscrape.cluster.memberNum=0 -promscrape.config=/path/config.yml ...
vmagent -promscrape.cluster.membersCount=2 -promscrape.cluster.memberNum=1 -promscrape.config=/path/config.yml ...
</code></pre></div>
<p>当 vmagent 在 Kubernetes 中运行时，可以将 <code>-promscrape.cluster.memberNum</code> 设置为 StatefulSet pod 名称，pod 名称必须以 <code>0 ... promscrape.cluster.memberNum-1</code> 范围内的数字结尾，例如，<code>-promscrape.cluster.memberNum=vmagent-0</code>。</p>
<p>默认情况下，每个抓取目标仅由集群中的单个 vmagent 实例抓取。如果需要在多个 vmagent 实例之间复制抓取目标，则可以通过 <code>-promscrape.cluster.replicationFactor</code> 参数设置为所需的副本数。例如，以下命令启动一个包含三个 vmagent 实例的集群，其中每个目标由两个 vmagent 实例抓取：</p>
<div class="highlight"><pre><span></span><code>vmagent -promscrape.cluster.membersCount=3 -promscrape.cluster.replicationFactor=2 -promscrape.cluster.memberNum=0 -promscrape.config=/path/to/config.yml ...
vmagent -promscrape.cluster.membersCount=3 -promscrape.cluster.replicationFactor=2 -promscrape.cluster.memberNum=1 -promscrape.config=/path/to/config.yml ...
vmagent -promscrape.cluster.membersCount=3 -promscrape.cluster.replicationFactor=2 -promscrape.cluster.memberNum=2 -promscrape.config=/path/to/config.yml ...
</code></pre></div>
<p>需要注意的是如果每个目标被多个 vmagent 实例抓取，则必须在 <code>-remoteWrite.url</code> 指向的远程存储上启用重复数据删除。</p>
<p>所以如果你抓取的监控目标非常大，那么我们建议使用 vmagent 集群模式，那么可以使用 StatefulSet 方式进行部署</p>
<div class="highlight"><pre><span></span><code># vmagent-sts.yaml
apiVersion: v1
kind: Service
metadata:
  name: vmagent
  namespace: kube-vm
  annotations:
    prometheus.io/scrape: &quot;true&quot;
    prometheus.io/port: &quot;8429&quot;
spec:
  selector:
    app: vmagent
  clusterIP: None
  ports:
    - name: http
      port: 8429
      targetPort: http
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: vmagent
  namespace: kube-vm
  labels:
    app: vmagent
spec:
  replicas: 2
  serviceName: vmagent
  selector:
    matchLabels:
      app: vmagent
  template:
    metadata:
      labels:
        app: vmagent
    spec:
      serviceAccountName: vmagent
      containers:
        - name: agent
          image: victoriametrics/vmagent:v1.77.0
          imagePullPolicy: IfNotPresent
          args:
            - -promscrape.config=/config/scrape.yml
            - -remoteWrite.tmpDataPath=/tmpData
            - -promscrape.cluster.membersCount=2
            # - -promscrape.cluster.replicationFactor=2 # 可以配置副本数
            - -promscrape.cluster.memberNum=$(POD_NAME)
            - -remoteWrite.url=http://vminsert:8480/insert/0/prometheus
            - -envflag.enable=true
            - -envflag.prefix=VM_
            - -loggerFormat=json
          ports:
            - name: http
              containerPort: 8429
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
          volumeMounts:
            - name: tmpdata
              mountPath: /tmpData
            - name: config
              mountPath: /config
      volumes:
        - name: config
          configMap:
            name: vmagent-config
  volumeClaimTemplates:
    - metadata:
        name: tmpdata
      spec:
        accessModes:
          - ReadWriteOnce
        storageClassName: nfs-client
        resources:
          requests:
            storage: 1Gi
</code></pre></div>
<p>我们这里就使用 StatefulSet 的形式来管理 vmagent，直接应用上面的资源即可：</p>
<div class="highlight"><pre><span></span><code># 先将前面示例中的 prometheus 停掉
☸ ➜ kubectl scale deploy prometheus --replicas=0 -n kube-vm
☸ ➜ kubectl apply -f vmagent-rbac.yaml
☸ ➜ kubectl apply -f vmagent-config.yaml
☸ ➜ kubectl apply -f vmagent-sts.yaml
☸ ➜ kubectl get pods -n kube-vm -l app=vmagent
NAME        READY   STATUS    RESTARTS   AGE
vmagent-0   1/1     Running   0          3m43s
vmagent-1   1/1     Running   0          2m9s
</code></pre></div>
<p>这里我们部署了两个 vmagent 实例来抓取监控指标，我们这里一共 3 个节点。</p>
<div class="highlight"><pre><span></span><code>☸ ➜ kubectl get nodes
NAME      STATUS   ROLES                  AGE   VERSION
master1   Ready    control-plane,master   44d   v1.22.8
node1     Ready    &lt;none&gt;                 44d   v1.22.8
node2     Ready    &lt;none&gt;                 44d   v1.22.8
</code></pre></div>
<p>所以两个 vmagent 实例会分别采集部分指标，我们可以通过查看日志来进行验证：</p>
<div class="highlight"><pre><span></span><code>☸ ➜ kubectl logs -f vmagent-0 -n kube-vm
# ......
{&quot;ts&quot;:&quot;2022-05-10T04:44:44.004Z&quot;,&quot;level&quot;:&quot;info&quot;,&quot;caller&quot;:&quot;VictoriaMetrics/lib/promscrape/scraper.go:393&quot;,&quot;msg&quot;:&quot;static_configs: added targets: 1, removed targets: 0; total targets: 1&quot;}
{&quot;ts&quot;:&quot;2022-05-10T04:44:44.006Z&quot;,&quot;level&quot;:&quot;info&quot;,&quot;caller&quot;:&quot;VictoriaMetrics/lib/promscrape/scraper.go:393&quot;,&quot;msg&quot;:&quot;kubernetes_sd_configs: added targets: 2, removed targets: 0; total targets: 2&quot;}
☸ ➜ kubectl logs -f vmagent-1 -n kube-vm
# ......
{&quot;ts&quot;:&quot;2022-05-10T04:46:17.893Z&quot;,&quot;level&quot;:&quot;info&quot;,&quot;caller&quot;:&quot;VictoriaMetrics/lib/promscrape/scraper.go:393&quot;,&quot;msg&quot;:&quot;kubernetes_sd_configs: added targets: 1, removed targets: 0; total targets: 1&quot;}
</code></pre></div>
<p>从日志可以看出 <code>vmagent-0</code> 实例发现了 2 个 targets，<code>vmagent-1</code> 实例发现了 1 个 targets，这也符合我们预期的。</p>
<p>接下来我们再新增其他内容的监控，比如 APIServer、容器等等，配置如下所示：</p>
<div class="highlight"><pre><span></span><code># vmagent-config2.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: vmagent-config
  namespace: kube-vm
data:
  scrape.yml: |
    global:
      scrape_interval: 15s
      scrape_timeout: 15s

    scrape_configs:

    - job_name: nodes
      kubernetes_sd_configs:
        - role: node
      relabel_configs:
      - source_labels: [__address__]
        regex: &quot;(.*):10250&quot;
        replacement: &quot;${1}:9111&quot;
        target_label: __address__
        action: replace
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)

    - job_name: apiserver
      scheme: https
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - action: keep
        regex: default;kubernetes;https
        source_labels:
        - __meta_kubernetes_namespace
        - __meta_kubernetes_service_name
        - __meta_kubernetes_endpoint_port_name

    - job_name: cadvisor
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
      kubernetes_sd_configs:
      - role: node
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - replacement: /metrics/cadvisor
        target_label: __metrics_path__

    - job_name: endpoints
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - action: drop
        regex: true
        source_labels:
        - __meta_kubernetes_pod_container_init
      - action: keep_if_equal
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_port
        - __meta_kubernetes_pod_container_port_number
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scrape
      - action: replace
        regex: (https?)
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scheme
        target_label: __scheme__
      - action: replace
        regex: (.+)
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_path
        target_label: __metrics_path__
      - action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        source_labels:
        - __address__
        - __meta_kubernetes_service_annotation_prometheus_io_port
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - source_labels:
        - __meta_kubernetes_pod_name
        target_label: pod
      - source_labels:
        - __meta_kubernetes_namespace
        target_label: namespace
      - source_labels:
        - __meta_kubernetes_service_name
        target_label: service
      - replacement: ${1}
        source_labels:
        - __meta_kubernetes_service_name
        target_label: job
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_node_name
        target_label: node
</code></pre></div>
<p>大部分的配置在前面 Prometheus 章节都介绍过了，核心就是通过 <code>relabel_configs</code> 来控制抓取的任务，vmagent 是兼容传统的 prometheus 重新标记规则的，但也有一些独特的 action，比如上面配置中我们使用了一个 <code>keep_if_equal</code> 的操作，该操作的意思是如果指定的标签值相等则将该条数据保留下来。</p>
<p>有时，如果某个指标包含两个具有相同值的标签，则需要删除它。这可以通过 vmagent 支持的 <code>drop_if_equal</code> 操作来完成。例如，如果以下 relabel 规则包含 <code>real_port</code> 和 <code>required_port</code> 的相同标签值，则它会删除指标：</p>
<div class="highlight"><pre><span></span><code>- action: drop_if_equal
  source_labels: [real_port, needed_port]
</code></pre></div>
<p>该规则将删除以下指标：<code>foo{real_port="123",needed_port="123"}</code>，但会保留以下指标：<code>foo{real_port="123",needed_port="456"}</code>。</p>
<p>有时可能需要只对指标子集应用 relabel，在这种情况下，可以将 <code>if</code> 选项添加到 <code>relabel_configs</code> 规则中，例如以下规则仅将 <code>{foo="bar"}</code> 标签添加到与 <code>metric{label=~"x|y"}</code> 序列选择器匹配的指标：</p>
<div class="highlight"><pre><span></span><code>- if: &#39;metric{label=~&quot;x|y&quot;}&#39;
  target_label: &quot;foo&quot;
  replacement: &quot;bar&quot;
</code></pre></div>
<p><code>if</code> 选项可以简化传统的 <code>relabel_configs</code> 规则，例如，以下规则可以删除与 <code>foo{bar="baz"}</code> 序列选择器匹配的指标：</p>
<div class="highlight"><pre><span></span><code>- if: &#39;foo{bar=&quot;baz&quot;}&#39;
  action: drop
</code></pre></div>
<p>这相当于以下传统的规则：</p>
<div class="highlight"><pre><span></span><code>- action: drop
  source_labels: [__name__, bar]
  regex: &quot;foo;baz&quot;
</code></pre></div>
<p>不过需要注意的是 Prometheus 还不支持 <code>if</code> 选项，现在只支持 VictoriaMetrics。</p>
<p>现在更新 vmagent 的配置。</p>
<div class="highlight"><pre><span></span><code>☸ ➜ kubectl apply -f vmagent-config2.yaml
</code></pre></div>
<p>配置刷新有两种方式：</p>
<ul>
<li>发送 SUGHUP 信号给 vmagent 进程</li>
<li>向 <code>http://vmagent:8429/-/reload</code> 发送一个 http 请求</li>
</ul>
<p>刷新后就可以开始采集上面的指标了，同样我们也可以通过 <code>http://vmselect/select/0/vmui/</code> 来访问 vmui，比如现在我们来查询 pod 的内存使用率，可以使用如下的查询语句：</p>
<div class="highlight"><pre><span></span><code>sum(container_memory_working_set_bytes{image!=&quot;&quot;}) by(namespace, pod) / sum(container_spec_memory_limit_bytes{image!=&quot;&quot;}) by(namespace, pod) * 100 != +inf
</code></pre></div>
<p><img alt="vmui" src="https://picdn.youdianzhishi.com/images/1652176438073.jpg" /></p>
<p>vmagent 作为采集指标重要的一环，当然对它的监控也不可少。vmagent 通过 <code>http://vmagent:8429/metrics</code> 暴露了很多指标，如 <code>vmagent_remotewrite_conns</code> 远程存储连接，<code>vm_allowed_memory_bytes</code> 可使用的内存大小，我们把一些重要的指标收集起来，通过 Grafana 进行展示，能够更好的帮助我们分析 vmagent 的状态。</p>
<p>我们可以使用 <a href="https://grafana.com/grafana/dashboards/12683">https://grafana.com/grafana/dashboards/12683</a> 来展示 vmagent 的状态。</p>
<p><img alt="vmagent grafana" src="https://picdn.youdianzhishi.com/images/1652177657145.png" /></p>
<p>此外如果想要查看 vmagent 的抓取的 targets，也通过通过 vmagent 提供的简单页面查看，不过只能查看到指定 vmagent 的，不能直接查看所有的 targets。</p>
<p><img alt="vmagent targets" src="https://picdn.youdianzhishi.com/images/1652342750757.jpg" /></p>
<h2 id="vmalert">vmalert<a class="headerlink" href="#vmalert" title="Permanent link">&para;</a></h2>
<p>前面我们已经介绍了可以使用 vmagent 代替 prometheus 抓取监控指标数据，要想完全替换 prometheus 还有一个非常重要的部分就是报警模块，之前我们都是在 prometheus 中定义报警规则评估后发送给 alertmanager 的，同样对应到 vm 中也有一个专门来处理报警的模块：vmalert。</p>
<p>vmalert 会针对 <code>-datasource.url</code> 地址执行配置的报警或记录规则，然后可以将报警发送给 <code>-notifier.url</code> 配置的 Alertmanager，记录规则结果会通过远程写入的协议进行保存，所以需要配置 <code>-remoteWrite.url</code>。</p>
<h3 id="_4">特性<a class="headerlink" href="#_4" title="Permanent link">&para;</a></h3>
<ul>
<li>与 VictoriaMetrics TSDB 集成</li>
<li>VictoriaMetrics MetricsQL 支持和表达式验证</li>
<li>Prometheus 告警规则定义格式支持</li>
<li>与 Alertmanager 集成</li>
<li>在重启时可以保持报警状态</li>
<li>Graphite 数据源可用于警报和记录规则</li>
<li>支持记录和报警规则重放</li>
<li>非常轻量级，没有额外的依赖</li>
</ul>
<p>要开始使用 vmalert，需要满足以下条件：</p>
<ul>
<li>报警规则列表：要执行的 PromQL/MetricsQL 表达式</li>
<li>数据源地址：可访问的 VictoriaMetrics 实例，用于规则执行</li>
<li>通知程序地址：可访问的 Alertmanager 实例，用于处理，汇总警报和发送通知</li>
</ul>
<h3 id="_5">安装<a class="headerlink" href="#_5" title="Permanent link">&para;</a></h3>
<p>首先需要安装一个 Alertmanager 用来接收报警信息，前面章节中我们已经详细讲解过了，这里不再赘述了，对应的资源清单如下所示：</p>
<div class="highlight"><pre><span></span><code># alertmanager.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: alert-config
  namespace: kube-vm
data:
  config.yml: |-
    global:
      resolve_timeout: 5m
      smtp_smarthost: &#39;smtp.163.com:465&#39;
      smtp_from: &#39;xxx@163.com&#39;
      smtp_auth_username: &#39;xxx@163.com&#39;
      smtp_auth_password: &#39;&lt;auth code&gt;&#39;  # 使用网易邮箱的授权码
      smtp_hello: &#39;163.com&#39;
      smtp_require_tls: false
    route:
      group_by: [&#39;severity&#39;, &#39;source&#39;]
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 24h
      receiver: email
    receivers:
    - name: &#39;email&#39;
      email_configs:
      - to: &#39;517554016@qq.com&#39;
        send_resolved: true
---
apiVersion: v1
kind: Service
metadata:
  name: alertmanager
  namespace: kube-vm
  labels:
    app: alertmanager
spec:
  selector:
    app: alertmanager
  type: NodePort
  ports:
    - name: web
      port: 9093
      targetPort: http
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: alertmanager
  namespace: kube-vm
  labels:
    app: alertmanager
spec:
  selector:
    matchLabels:
      app: alertmanager
  template:
    metadata:
      labels:
        app: alertmanager
    spec:
      volumes:
        - name: cfg
          configMap:
            name: alert-config
      containers:
        - name: alertmanager
          image: prom/alertmanager:v0.24.0
          imagePullPolicy: IfNotPresent
          args:
            - &quot;--config.file=/etc/alertmanager/config.yml&quot;
          ports:
            - containerPort: 9093
              name: http
          volumeMounts:
            - mountPath: &quot;/etc/alertmanager&quot;
              name: cfg
</code></pre></div>
<p>Alertmanager 这里我们只配置了一个默认的路由规则，根据 <code>severity</code>、<code>source</code> 两个标签进行分组，然后将触发的报警发送到 email 接收器中去。</p>
<p>接下来需要添加用于报警的规则配置，配置方式和 Prometheus 一样的：</p>
<div class="highlight"><pre><span></span><code># vmalert-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: vmalert-config
  namespace: kube-vm
data:
  record.yaml: |
    groups:
    - name: record
      rules:
      - record: job:node_memory_MemFree_bytes:percent  # 记录规则名称
        expr: 100 - (100 * node_memory_MemFree_bytes / node_memory_MemTotal_bytes)
  pod.yaml: |
    groups:
    - name: pod
      rules:
      - alert: PodMemoryUsage
        expr: sum(container_memory_working_set_bytes{pod!=&quot;&quot;}) BY (instance, pod)  / sum(container_spec_memory_limit_bytes{pod!=&quot;&quot;} &gt; 0) BY (instance, pod) * 100 &gt; 60
        for: 2m
        labels:
          severity: warning
          source: pod
        annotations:
          summary: &quot;Pod {{ $labels.pod }} High Memory usage detected&quot;
          description: &quot;{{$labels.instance}}: Pod {{ $labels.pod }} Memory usage is above 60% (current value is: {{ $value }})&quot;
  node.yaml: |
    groups:
    - name: node
      rules:  # 具体的报警规则
      - alert: NodeMemoryUsage  # 报警规则的名称
        expr: (node_memory_MemTotal_bytes - (node_memory_MemFree_bytes + node_memory_Buffers_bytes + node_memory_Cached_bytes)) / node_memory_MemTotal_bytes * 100 &gt; 30
        for: 1m
        labels:
          source: node
          severity: critical
        annotations:
          summary: &quot;Node {{$labels.instance}} High Memory usage detected&quot;
          description: &quot;{{$labels.instance}}: Memory usage is above 30% (current value is: {{ $value }})&quot;
</code></pre></div>
<p>这里我们添加了一条记录规则，两条报警规则，更多报警规则配置可参考 <a href="https://awesome-prometheus-alerts.grep.to/">https://awesome-prometheus-alerts.grep.to/</a>。</p>
<p>然后就可以部署 vmalert 组件服务了：</p>
<div class="highlight"><pre><span></span><code># vmalert.yaml
apiVersion: v1
kind: Service
metadata:
  name: vmalert
  namespace: kube-vm
  labels:
    app: vmalert
spec:
  ports:
    - name: vmalert
      port: 8080
      targetPort: 8080
  type: NodePort
  selector:
    app: vmalert
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vmalert
  namespace: kube-vm
  labels:
    app: vmalert
spec:
  selector:
    matchLabels:
      app: vmalert
  template:
    metadata:
      labels:
        app: vmalert
    spec:
      containers:
        - name: vmalert
          image: victoriametrics/vmalert:v1.77.0
          imagePullPolicy: IfNotPresent
          args:
            - -rule=/etc/ruler/*.yaml
            - -datasource.url=http://vmselect.kube-vm.svc.cluster.local:8481/select/0/prometheus
            - -notifier.url=http://alertmanager.kube-vm.svc.cluster.local:9093
            - -remoteWrite.url=http://vminsert.kube-vm.svc.cluster.local:8480/insert/0/prometheus
            - -evaluationInterval=15s
            - -httpListenAddr=0.0.0.0:8080
          volumeMounts:
            - mountPath: /etc/ruler/
              name: ruler
              readOnly: true
      volumes:
        - configMap:
            name: vmalert-config
          name: ruler
</code></pre></div>
<p>上面的资源清单中将报警规则以 volumes 的形式挂载到了容器中，通过 <code>-rule</code> 指定了规则文件路径，<code>-datasource.url</code> 指定了 vmselect 的路径，<code>-notifier.url</code> 指定了 Alertmanager 的地址，其中 <code>-evaluationInterval</code> 参数用来指定评估的频率的，由于我们这里添加了记录规则，所以还需要通过 <code>-remoteWrite.url</code> 指定一个远程写入的地址。</p>
<p>直接创建上面的资源清单即可完成部署。</p>
<div class="highlight"><pre><span></span><code>☸ ➜ kubectl apply -f https://p8s.io/docs/victoriametrics/manifests/alertmanager.yaml
☸ ➜ kubectl apply -f https://p8s.io/docs/victoriametrics/manifests/vmalert-config.yaml
☸ ➜ kubectl apply -f https://p8s.io/docs/victoriametrics/manifests/vmalert.yaml
☸ ➜ kubectl get pods -n kube-vm -l app=alertmanager
NAME                           READY   STATUS    RESTARTS   AGE
alertmanager-d88d95b4f-z2j8g   1/1     Running   0          30m
☸ ➜ kubectl get svc -n kube-vm -l app=alertmanager
NAME           TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
alertmanager   NodePort   10.100.230.2   &lt;none&gt;        9093:31282/TCP   31m
☸ ➜ kubectl get pods -n kube-vm -l app=vmalert
NAME                       READY   STATUS    RESTARTS   AGE
vmalert-866674b966-675nb   1/1     Running   0          7m17s
☸ ➜ kubectl get svc -n kube-vm -l app=vmalert
NAME      TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
vmalert   NodePort   10.104.193.183   &lt;none&gt;        8080:30376/TCP   22m
</code></pre></div>
<p>部署成功后，如果有报警规则达到了阈值就会触发报警，我们可以通过 Alertmanager 页面查看触发的报警规则：</p>
<p><img alt="alertmanager" src="https://picdn.youdianzhishi.com/images/1652329519753.png" /></p>
<p>同样 vmalert 也提供了一个简单的页面，可以查看所有的 Groups：</p>
<p><img alt="groups" src="https://picdn.youdianzhishi.com/images/1652329798064.png" /></p>
<p>也可以查看到报警规则列表的状态：</p>
<p><img alt="alerts" src="https://picdn.youdianzhishi.com/images/1652329833032.png" /></p>
<p>还可以查看到具体的一条报警规则的详细信息，如下所示：</p>
<p><img alt="alert detail" src="https://picdn.youdianzhishi.com/images/1652329883585.png" /></p>
<p>报警规则触发后怎么发送，发送到哪个接收器就是 Alertmanager 决定的了。</p>
<p>同样的上面我们添加的记录规则会通过 remote write 传递给 vminsert 保留下来，所以我们也可以通过 vmselect 查询到。</p>
<p><img alt="记录规则" src="https://picdn.youdianzhishi.com/images/1652340892425.jpg" /></p>
<p>到这里基本上我们就完成了使用 vm 代替 prometheus 来进行监控报警了，vmagent 采集监控指标，vmalert 用于报警监控，vmstorage 存储指标数据，vminsert 接收指标数据，vmselect 查询指标数据，已经完全可以不使用 prometheus 了，而且性能非常高，所需资源也比 prometheus 低很多。</p>
<h2 id="vm-operator">vm-operator<a class="headerlink" href="#vm-operator" title="Permanent link">&para;</a></h2>
<p>Operator 我们知道是 Kubernetes 的一大杀器，可以大大简化应用的安装、配置和管理，同样对于 VictoriaMetrics 官方也开发了一个对应的 Operator 来进行管理 - vm-operator，它的设计和实现灵感来自 prometheus-operator（后面会讲解），它是管理应用程序监控配置的绝佳工具。</p>
<p>vm-operator 定义了如下一些 CRD：</p>
<ul>
<li><code>VMServiceScrape</code>：定义从 Service 支持的 Pod 中抓取指标配置</li>
<li><code>VMPodScrape</code>：定义从 Pod 中抓取指标配置</li>
<li><code>VMRule</code>：定义报警和记录规则</li>
<li><code>VMProbe</code>：使用 blackbox exporter 为目标定义探测配置</li>
</ul>
<p>此外该 Operator 默认还可以识别 prometheus-operator 中的 <code>ServiceMonitor</code>、<code>PodMonitor</code>、<code>PrometheusRule</code> 和 <code>Probe</code> 对象，还允许你使用 CRD 对象来管理 Kubernetes 集群内的 VM 应用。</p>
<h3 id="_6">安装<a class="headerlink" href="#_6" title="Permanent link">&para;</a></h3>
<p>vm-operator 提供了 Helm Charts 包，所以可以使用 Helm 来进行一键安装：</p>
<div class="highlight"><pre><span></span><code>☸ ➜ helm repo add vm https://victoriametrics.github.io/helm-charts/
☸ ➜ helm repo update
</code></pre></div>
<p>根据自己的需要定制 values 值，默认的 <code>values.yaml</code> 可以通过下面的命令获得：</p>
<div class="highlight"><pre><span></span><code>☸ ➜ helm show values vm/victoria-metrics-operator &gt; values.yaml
</code></pre></div>
<p>我们这里只对下面的内容做了修改：</p>
<div class="highlight"><pre><span></span><code># values.yaml
operator:
  # -- 默认情况下，vm-operator会转换prometheus-operator对象
  disable_prometheus_converter: false
  # -- 默认情况下，vm-operator会为它的对象创建psp
  psp_auto_creation_enabled: false
  # -- 启用转换后的 prometheus-operator 对象的所有权引用，如果删除 prometheus 对象，它将删除相应的 victoria-metrics 对象。
  enable_converter_ownership: false
  # -- Enables custom config-reloader, bundled with operator.
  # It should reduce  vmagent and vmauth config sync-time and make it predictable.
  useCustomConfigReloader: true
# -- 是否开启资源校验的准入控制器(生产环境建议开启)
# admissionWebhooks:
#   # -- Enables validation webhook.
#   enabled: false
#   # -- What to do in case, when operator not available to validate request.
#   policy: Fail
#   # -- Enables custom ca bundle, if you are not using cert-manager.
#   # -- in case of custom ca, you have to create secret - {{chart-name}}-validation
#   # -- with keys: tls.key, tls.crt, ca.crt
#   caBundle: &quot;&quot;
#   certManager:
#     # -- Enables cert creation and injection by cert-manager.
#     enabled: false
#     # --If needed, provide own issuer. Operator will create self-signed if empty.
#     issuer: {}
</code></pre></div>
<p>然后使用下面的命令即可一键安装 vm-operator：</p>
<div class="highlight"><pre><span></span><code>☸ ➜ helm upgrade --install victoria-metrics-operator vm/victoria-metrics-operator -f values.yaml -n vm-operator --create-namespace
NAME: victoria-metrics-operator
LAST DEPLOYED: Tue May 17 15:51:40 2022
NAMESPACE: vm-operator
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
victoria-metrics-operator has been installed. Check its status by running:
  kubectl --namespace vm-operator get pods -l &quot;app.kubernetes.io/instance=victoria-metrics-operator&quot;

Get more information on https://github.com/VictoriaMetrics/helm-charts/tree/master/charts/victoria-metrics-operator.
See &quot;Getting started guide for VM Operator&quot; on https://docs.victoriametrics.com/guides/getting-started-with-vm-operator.html .
</code></pre></div>
<p>安装完成后可以查看 vm-operator 的状态来验证是否安装成功：</p>
<div class="highlight"><pre><span></span><code>☸ ➜ helm ls -n vm-operator
NAME                            NAMESPACE       REVISION        UPDATED                                 STATUS       CHART                           APP VERSION
victoria-metrics-operator       vm-operator     1               2022-05-17 15:53:14.60667 +0800 CST     deployed     victoria-metrics-operator-0.9.0 0.24.0
☸ ➜ kubectl --namespace vm-operator get pods -l &quot;app.kubernetes.io/instance=victoria-metrics-operator&quot;
NAME                                        READY   STATUS    RESTARTS   AGE
victoria-metrics-operator-d467cf69c-glh6v   1/1     Running   0          2m58s
</code></pre></div>
<h3 id="vm">安装 VM 集群<a class="headerlink" href="#vm" title="Permanent link">&para;</a></h3>
<p>Operator 安装完成后会包含如下所示的一些 CRD：</p>
<div class="highlight"><pre><span></span><code>☸ ➜ kubectl get crd |grep victoriametrics
vmagents.operator.victoriametrics.com                2022-05-17T07:51:42Z
vmalertmanagerconfigs.operator.victoriametrics.com   2022-05-17T07:51:42Z
vmalertmanagers.operator.victoriametrics.com         2022-05-17T07:51:42Z
vmalerts.operator.victoriametrics.com                2022-05-17T07:51:42Z
vmauths.operator.victoriametrics.com                 2022-05-17T07:51:42Z
vmclusters.operator.victoriametrics.com              2022-05-17T07:51:42Z
vmnodescrapes.operator.victoriametrics.com           2022-05-17T07:51:42Z
vmpodscrapes.operator.victoriametrics.com            2022-05-17T07:51:42Z
vmprobes.operator.victoriametrics.com                2022-05-17T07:51:42Z
vmrules.operator.victoriametrics.com                 2022-05-17T07:51:42Z
vmservicescrapes.operator.victoriametrics.com        2022-05-17T07:51:42Z
vmsingles.operator.victoriametrics.com               2022-05-17T07:51:42Z
vmstaticscrapes.operator.victoriametrics.com         2022-05-17T07:51:42Z
vmusers.operator.victoriametrics.com                 2022-05-17T07:51:42Z
</code></pre></div>
<p>比如现在我们要来部署 VM，如果只是想要单节点模式则可以直接使用 <code>VMSingle</code> 对象，如果要部署一套 VM 的集群则可以直接使用 <code>VMCluster</code> 来定义一个对象即可，完全不需要我们去手动创建各个组件，Operator 会根据我们的定义去帮我们拉起一套集群起来。</p>
<p>比如这里我们定义一个如下所示的 <code>VMCluster</code> 对象：</p>
<div class="highlight"><pre><span></span><code># vmcluster-demo.yaml
apiVersion: operator.victoriametrics.com/v1beta1
kind: VMCluster
metadata:
  name: vmcluster-demo
spec:
  replicationFactor: 1
  retentionPeriod: &quot;1w&quot;
  vmstorage:
    replicaCount: 2
    storage:
      volumeClaimTemplate:
        spec:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 10G
          storageClassName: nfs-client
    storageDataPath: /vm-data
  vmselect:
    replicaCount: 2
    cacheMountPath: /cache
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: nfs-client
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 1G
  vminsert:
    replicaCount: 2
</code></pre></div>
<p>这里我们通过 <code>spec.retentionPeriod</code> 指定了数据保留的时长为 1 周，<code>replicaCount</code> 用来指定各个组件的副本数为 2，通过 <code>storage.volumeClaimTemplate</code> 指定了数据持久化的 PVC 模板，整个对象可配置的属性我们可以通过 <code>kubectl explain</code> 来获取：</p>
<div class="highlight"><pre><span></span><code>☸ ➜ kubectl explain VMCluster.spec
KIND:     VMCluster
VERSION:  operator.victoriametrics.com/v1beta1

RESOURCE: spec &lt;Object&gt;

DESCRIPTION:
     VMClusterSpec defines the desired state of VMCluster

FIELDS:
   clusterVersion       &lt;string&gt;
     ClusterVersion defines default images tag for all components. it can be
     overwritten with component specific image.tag value.

   imagePullSecrets     &lt;[]Object&gt;
     ImagePullSecrets An optional list of references to secrets in the same
     namespace to use for pulling images from registries see
     http://kubernetes.io/docs/user-guide/images#specifying-imagepullsecrets-on-a-pod

   podSecurityPolicyName        &lt;string&gt;
     PodSecurityPolicyName - defines name for podSecurityPolicy in case of empty
     value, prefixedName will be used.

   replicationFactor    &lt;integer&gt;
     ReplicationFactor defines how many copies of data make among distinct
     storage nodes

   retentionPeriod      &lt;string&gt; -required-
     RetentionPeriod for the stored metrics Note VictoriaMetrics has data/ and
     indexdb/ folders metrics from data/ removed eventually as soon as partition
     leaves retention period reverse index data at indexdb rotates once at the
     half of configured retention period
     https://docs.victoriametrics.com/Single-server-VictoriaMetrics.html#retention

   serviceAccountName   &lt;string&gt;
     ServiceAccountName is the name of the ServiceAccount to use to run the
     VMSelect Pods.

   vminsert     &lt;Object&gt;

   vmselect     &lt;Object&gt;

   vmstorage    &lt;Object&gt;
</code></pre></div>
<p>同样要想获取组件可以定义的属性也可以通过该方式来获取，比如查看 <code>vmstorage</code> 对象可以配置的属性：</p>
<div class="highlight"><pre><span></span><code>☸ ➜ kubectl explain VMCluster.spec.vmstorage
KIND:     VMCluster
VERSION:  operator.victoriametrics.com/v1beta1

RESOURCE: vmstorage &lt;Object&gt;

DESCRIPTION:
     &lt;empty&gt;

FIELDS:
   affinity     &lt;&gt;
     Affinity If specified, the pod&#39;s scheduling constraints.

   configMaps   &lt;[]string&gt;
     ConfigMaps is a list of ConfigMaps in the same namespace as the VMSelect
     object, which shall be mounted into the VMSelect Pods. The ConfigMaps are
     mounted into /etc/vm/configs/&lt;configmap-name&gt;.

   containers   &lt;[]&gt;
     Containers property allows to inject additions sidecars or to patch
     existing containers. It can be useful for proxies, backup, etc.

   dnsConfig    &lt;Object&gt;
     Specifies the DNS parameters of a pod. Parameters specified here will be
     merged to the generated DNS configuration based on DNSPolicy.

   dnsPolicy    &lt;string&gt;
     DNSPolicy sets DNS policy for the pod

   extraArgs    &lt;map[string]string&gt;

   extraEnvs    &lt;[]&gt;
     ExtraEnvs that will be added to VMSelect pod

   hostNetwork  &lt;boolean&gt;
     HostNetwork controls whether the pod may use the node network namespace

   image        &lt;Object&gt;
     Image - docker image settings for VMStorage

   initContainers       &lt;[]&gt;
     InitContainers allows adding initContainers to the pod definition. Those
     can be used to e.g. fetch secrets for injection into the VMSelect
     configuration from external sources. Any errors during the execution of an
     initContainer will lead to a restart of the Pod. More info:
     https://kubernetes.io/docs/concepts/workloads/pods/init-containers/ Using
     initContainers for any use case other then secret fetching is entirely
     outside the scope of what the maintainers will support and by doing so, you
     accept that this behaviour may break at any time without notice.

   livenessProbe        &lt;&gt;
     LivenessProbe that will be added CRD pod

   logFormat    &lt;string&gt;
     LogFormat for VMSelect to be configured with. default or json

   logLevel     &lt;string&gt;
     LogLevel for VMSelect to be configured with.

   maintenanceInsertNodeIDs     &lt;[]integer&gt;
     MaintenanceInsertNodeIDs - excludes given node ids from insert requests
     routing, must contain pod suffixes - for pod-0, id will be 0 and etc. lets
     say, you have pod-0, pod-1, pod-2, pod-3. to exclude pod-0 and pod-3 from
     insert routing, define nodeIDs: [0,3]. Useful at storage expanding, when
     you want to rebalance some data at cluster.

   maintenanceSelectNodeIDs     &lt;[]integer&gt;
     MaintenanceInsertNodeIDs - excludes given node ids from select requests
     routing, must contain pod suffixes - for pod-0, id will be 0 and etc.

   name &lt;string&gt;
     Name is deprecated and will be removed at 0.22.0 release

   nodeSelector &lt;map[string]string&gt;
     NodeSelector Define which Nodes the Pods are scheduled on.

   podDisruptionBudget  &lt;Object&gt;
     PodDisruptionBudget created by operator

   podMetadata  &lt;Object&gt;
     PodMetadata configures Labels and Annotations which are propagated to the
     VMSelect pods.

   port &lt;string&gt;
     Port for health check connetions

   priorityClassName    &lt;string&gt;
     Priority class assigned to the Pods

   readinessProbe       &lt;&gt;
     ReadinessProbe that will be added CRD pod

   replicaCount &lt;integer&gt; -required-
     ReplicaCount is the expected size of the VMStorage cluster. The controller
     will eventually make the size of the running cluster equal to the expected
     size.

   resources    &lt;Object&gt;
     Resources container resource request and limits,
     https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/

   rollingUpdateStrategy        &lt;string&gt;
     RollingUpdateStrategy defines strategy for application updates Default is
     OnDelete, in this case operator handles update process Can be changed for
     RollingUpdate

   runtimeClassName     &lt;string&gt;
     RuntimeClassName - defines runtime class for kubernetes pod.
     https://kubernetes.io/docs/concepts/containers/runtime-class/

   schedulerName        &lt;string&gt;
     SchedulerName - defines kubernetes scheduler name

   secrets      &lt;[]string&gt;
     Secrets is a list of Secrets in the same namespace as the VMSelect object,
     which shall be mounted into the VMSelect Pods. The Secrets are mounted into
     /etc/vm/secrets/&lt;secret-name&gt;.

   securityContext      &lt;&gt;
     SecurityContext holds pod-level security attributes and common container
     settings. This defaults to the default PodSecurityContext.

   serviceScrapeSpec    &lt;&gt;
     ServiceScrapeSpec that will be added to vmselect VMServiceScrape spec

   serviceSpec  &lt;Object&gt;
     ServiceSpec that will be create additional service for vmstorage

   startupProbe &lt;&gt;
     StartupProbe that will be added to CRD pod

   storage      &lt;Object&gt;
     Storage - add persistent volume for StorageDataPath its useful for
     persistent cache

   storageDataPath      &lt;string&gt;
     StorageDataPath - path to storage data

   terminationGracePeriodSeconds        &lt;integer&gt;
     TerminationGracePeriodSeconds period for container graceful termination

   tolerations  &lt;[]Object&gt;
     Tolerations If specified, the pod&#39;s tolerations.

   topologySpreadConstraints    &lt;[]&gt;
     TopologySpreadConstraints embedded kubernetes pod configuration option,
     controls how pods are spread across your cluster among failure-domains such
     as regions, zones, nodes, and other user-defined topology domains
     https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/

   vmBackup     &lt;Object&gt;
     VMBackup configuration for backup

   vmInsertPort &lt;string&gt;
     VMInsertPort for VMInsert connections

   vmSelectPort &lt;string&gt;
     VMSelectPort for VMSelect connections

   volumeMounts &lt;[]Object&gt;
     VolumeMounts allows configuration of additional VolumeMounts on the output
     Deployment definition. VolumeMounts specified will be appended to other
     VolumeMounts in the VMSelect container, that are generated as a result of
     StorageSpec objects.

   volumes      &lt;[]&gt;
     Volumes allows configuration of additional volumes on the output Deployment
     definition. Volumes specified will be appended to other volumes that are
     generated as a result of StorageSpec objects.
</code></pre></div>
<p>直接应用上面定义的对象：</p>
<div class="highlight"><pre><span></span><code>☸ ➜ kubectl apply -f vmcluster-demo.yaml
☸ ➜ kubectl get vmcluster
NAME             INSERT COUNT   STORAGE COUNT   SELECT COUNT   AGE     STATUS
vmcluster-demo   2              2               2              7m21s   expanding
</code></pre></div>
<p>应用后 vm-operator 会 watch 到我们创建了该 CRD 对象，然后会根据我们的定义去自动创建对应的 VM 集群，也就是前面提到的几个组件服务：</p>
<div class="highlight"><pre><span></span><code>☸ ➜ kubectl get pods
NAME                                       READY   STATUS    RESTARTS      AGE
vminsert-vmcluster-demo-84956d98b5-5ckft   1/1     Running   0             93s
vminsert-vmcluster-demo-84956d98b5-kpcj6   1/1     Running   0             93s
vmselect-vmcluster-demo-0                  1/1     Running   0             3m7s
vmselect-vmcluster-demo-1                  1/1     Running   0             3m7s
vmstorage-vmcluster-demo-0                 1/1     Running   0             4m54s
vmstorage-vmcluster-demo-1                 1/1     Running   0             4m54s
☸ ➜ kubectl get svc
NAME                       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
vminsert-vmcluster-demo    ClusterIP   10.102.145.24   &lt;none&gt;        8480/TCP                     4m57s
vmselect-vmcluster-demo    ClusterIP   None            &lt;none&gt;        8481/TCP                     6m31s
vmstorage-vmcluster-demo   ClusterIP   None            &lt;none&gt;        8482/TCP,8400/TCP,8401/TCP   8m18s
</code></pre></div>
<p>我们只通过定义简单的 <code>VMCluster</code> 对象就可以来管理 VM 集群了，是不是非常方便，特别是当你组件副本数量非常多的时候不需要我们去手动配置 <code>-storageNode</code> 参数了。</p>
<p>现在 VM 集群安装成功了，但是现在还没有任何数据，所以还需要去配置监控指标的抓取，这里我们可以直接去创建一个 <code>VMAgent</code> 对象即可，创建一个如下所示的对象：</p>
<div class="highlight"><pre><span></span><code># vmagent-demo.yaml
apiVersion: operator.victoriametrics.com/v1beta1
kind: VMAgent
metadata:
  name: vmagent-demo
spec:
  serviceScrapeNamespaceSelector: {}
  podScrapeNamespaceSelector: {}
  podScrapeSelector: {}
  serviceScrapeSelector: {}
  nodeScrapeSelector: {}
  nodeScrapeNamespaceSelector: {}
  staticScrapeSelector: {}
  staticScrapeNamespaceSelector: {}
  replicaCount: 1
  remoteWrite:
    - url: &quot;http://vminsert-vmcluster-demo.default.svc.cluster.local:8480/insert/0/prometheus/api/v1/write&quot;
</code></pre></div>
<p>同样要获取 <code>VMAgent</code> 的所以可配置的属性可以通过 <code>kubectl explain VMAgent.spec</code> 来获取，这里最主要的配置就是通过 <code>remoteWrite.url</code> 来指定远程写入的 URL 地址，也就是 <code>vminsert</code> 组件的服务地址，其他几个属性可以用来对要抓取的指标进行过滤。</p>
<p>直接应用上面的 <code>VMAgent</code> 对象即可开始抓取监控数据：</p>
<div class="highlight"><pre><span></span><code>☸ ➜ kubectl apply -f vmagent-demo.yaml
☸ ➜ kubectl get vmagent
NAME           AGE
vmagent-demo   6s
</code></pre></div>
<p>创建后 vm-operator 会根据对应的描述创建一个对应的 <code>vmagent</code> 实例：</p>
<div class="highlight"><pre><span></span><code>☸ ➜ kubectl get pods -l app.kubernetes.io/name=vmagent
NAME                                    READY   STATUS    RESTARTS   AGE
vmagent-vmagent-demo-6dcc7f9dfd-hxsff   2/2     Running   0          4m24s
</code></pre></div>
<p>可以看到 <code>vmagent</code> 有两个容器，一个是 <code>vmagent</code> 应用容器，另外一个是用于挂载 Secret 对象的 <code>config-reloader</code> 容器，它会 watch 配置的变化，并发送信号为 <code>vmagent</code> 重新加载配置，该 Secret 对象中就是定义的 <code>vmagent</code> 抓取指标的配置内容。</p>
<p>我们可以运行以下命令使 <code>vmagent</code> 的端口可以从本地机器上访问。</p>
<div class="highlight"><pre><span></span><code>☸ ➜ kubectl port-forward svc/vmagent-vmagent-demo 8429:8429
Forwarding from 127.0.0.1:8429 -&gt; 8429
Forwarding from [::1]:8429 -&gt; 8429
</code></pre></div>
<p>我们可以在浏览器中访问 http://127.0.0.1:8429/targets 来检查 <code>vmagent</code> 采集的集群指标：</p>
<p><img alt="vmagent metrics" src="https://picdn.youdianzhishi.com/images/1652776636023.jpg" /></p>
<p><code>vmagent</code> 会通过 Kubernetes 服务发现去获取需要抓取的目标，此服务发现由 vm-operator 控制。</p>
<h3 id="vm_1">验证 VM 集群<a class="headerlink" href="#vm_1" title="Permanent link">&para;</a></h3>
<p>接下来我们安装 Grafana 来验证 VM 集群，这里为了简单我们就直接使用 Helm Chart 进行安装：</p>
<div class="highlight"><pre><span></span><code>☸ ➜ helm repo add grafana https://grafana.github.io/helm-charts
☸ ➜ helm repo update
</code></pre></div>
<p>我们可以在 values 中提前定义数据源和内置一些 dashboard，如下所示：</p>
<div class="highlight"><pre><span></span><code>cat &lt;&lt;EOF | helm install grafana grafana/grafana -f -
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        - name: victoriametrics
          type: prometheus
          orgId: 1
          url: http://vmselect-vmcluster-demo.default.svc.cluster.local:8481/select/0/prometheus/
          access: proxy
          isDefault: true
          updateIntervalSeconds: 10
          editable: true

  dashboardProviders:
   dashboardproviders.yaml:
     apiVersion: 1
     providers:
     - name: &#39;default&#39;
       orgId: 1
       folder: &#39;&#39;
       type: file
       disableDeletion: true
       editable: true
       options:
         path: /var/lib/grafana/dashboards/default

  dashboards:
    default:
      victoriametrics:
        gnetId: 11176
        revision: 18
        datasource: victoriametrics
      vmagent:
        gnetId: 12683
        revision: 7
        datasource: victoriametrics
      kubernetes:
        gnetId: 14205
        revision: 1
        datasource: victoriametrics
EOF
NAME: grafana
LAST DEPLOYED: Tue May 17 17:13:14 2022
NAMESPACE: default
STATUS: deployed
REVISION: 1
NOTES:
1. Get your &#39;admin&#39; user password by running:

   kubectl get secret --namespace default grafana -o jsonpath=&quot;{.data.admin-password}&quot; | base64 --decode ; echo

2. The Grafana server can be accessed via port 80 on the following DNS name from within your cluster:

   grafana.default.svc.cluster.local

   Get the Grafana URL to visit by running these commands in the same shell:

     export POD_NAME=$(kubectl get pods --namespace default -l &quot;app.kubernetes.io/name=grafana,app.kubernetes.io/instance=grafana&quot; -o jsonpath=&quot;{.items[0].metadata.name}&quot;)
     kubectl --namespace default port-forward $POD_NAME 3000

3. Login with the password from step 1 and the username: admin
#################################################################################
######   WARNING: Persistence is disabled!!! You will lose your data when   #####
######            the Grafana pod is terminated.                            #####
#################################################################################
</code></pre></div>
<p>安装完成后可以使用上面提示的命令在本地暴露 Grafana 服务：</p>
<div class="highlight"><pre><span></span><code>☸ ➜ export POD_NAME=$(kubectl get pods --namespace default -l &quot;app.kubernetes.io/name=grafana,app.kubernetes.io/instance=grafana&quot; -o jsonpath=&quot;{.items[0].metadata.name}&quot;)
     kubectl --namespace default port-forward $POD_NAME 3000
Forwarding from 127.0.0.1:3000 -&gt; 3000
Forwarding from [::1]:3000 -&gt; 3000
</code></pre></div>
<p>登录的用户名为 <code>admin</code>，密码可以通过下面的命令获取：</p>
<div class="highlight"><pre><span></span><code>☸ ➜ kubectl get secret --namespace default grafana -o jsonpath=&quot;{.data.admin-password}&quot; | base64 --decode ; echo
</code></pre></div>
<p>我们可以查看下 victoriametrics cluster 的 dashboard：</p>
<p><img alt="vmcluster" src="https://picdn.youdianzhishi.com/images/1652779882218.png" /></p>
<p>正常可以看到如下所示的页面：</p>
<p><img alt="vmcluster dashboard" src="https://picdn.youdianzhishi.com/images/1652780191173.png" /></p>
<p>这是因为默认情况下 <code>VMAgent</code> 会采集 VM 集群相关组件的指标，包括 <code>vmagent</code> 本身的，所以我们可以正常看到 VM 集群的 Dashboard，但是缺没有采集其他的指标，比如 node-exporter，我们可以在 Grafana 中导入 <code>16098</code> 这个 dashboard：</p>
<p><img alt="node-exporter" src="https://picdn.youdianzhishi.com/images/1652780481654.png" /></p>
<p>这个时候我们可以通过 <code>VMNodeScrape</code> 这个 CRD 对象来进行定义，<code>VMNodeScrape</code> 对象可以用来自动发现 Kubernetes 节点，创建如下所示的资源对象来采集 node-exporter 指标：</p>
<div class="highlight"><pre><span></span><code># vmnode-exporter-scrape.yaml
apiVersion: operator.victoriametrics.com/v1beta1
kind: VMNodeScrape
metadata:
  name: node-exporter
spec:
  path: /metrics
  port: &quot;9111&quot; # 指定 node-exporter 的端口
  scrape_interval: 15s
#   relabelConfigs：  # relabel配置
#   selector:  # 过滤节点
</code></pre></div>
<p>直接应用上面的对象即可：</p>
<div class="highlight"><pre><span></span><code>☸ ➜ kubectl apply -f vmnode-exporter-scrape.yaml
☸ ➜ kubectl get vmnodescrape
NAME            AGE
node-exporter   19s
</code></pre></div>
<p>创建后 vmagent 就会自动去识别该对象去对 node-exporter 进行抓取了：</p>
<p><img alt="node-exporter targets" src="https://picdn.youdianzhishi.com/images/1652781073840.png" /></p>
<p>这个时候再去查看 node-exporter 的 dashboard 就正常了：</p>
<p><img alt="node-exorter dashboard" src="https://picdn.youdianzhishi.com/images/1652781296628.png" /></p>
<p>此外还可以通过 <code>VMServiceScrape</code> 去定义要抓取的 Service 服务（Endpoints），它基于选择器为 <code>vmagent</code> 生成抓取配置，如果想要抓取没有定义 Service 的 Pod 的指标，则可以通过 <code>VMPodScrape</code> 来进行定义，同样还有报警相关的也都有相应的 CRD 来进行管理。vm-operator 大大降低了我们对 VM 集群的管理，非常推荐使用。</p>

              
            </article>
          </div>
        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" data-md-state="hidden">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"/></svg>
            回到页面顶部
          </a>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2022 lixie
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
      
      
    
    <a href="https://github.com/cnych" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
    
    
      
      
    
    <a href="https://hub.docker.com/r/cnych/" target="_blank" rel="noopener" title="hub.docker.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><path d="M349.9 236.3h-66.1v-59.4h66.1v59.4zm0-204.3h-66.1v60.7h66.1V32zm78.2 144.8H362v59.4h66.1v-59.4zm-156.3-72.1h-66.1v60.1h66.1v-60.1zm78.1 0h-66.1v60.1h66.1v-60.1zm276.8 100c-14.4-9.7-47.6-13.2-73.1-8.4-3.3-24-16.7-44.9-41.1-63.7l-14-9.3-9.3 14c-18.4 27.8-23.4 73.6-3.7 103.8-8.7 4.7-25.8 11.1-48.4 10.7H2.4c-8.7 50.8 5.8 116.8 44 162.1 37.1 43.9 92.7 66.2 165.4 66.2 157.4 0 273.9-72.5 328.4-204.2 21.4.4 67.6.1 91.3-45.2 1.5-2.5 6.6-13.2 8.5-17.1l-13.3-8.9zm-511.1-27.9h-66v59.4h66.1v-59.4zm78.1 0h-66.1v59.4h66.1v-59.4zm78.1 0h-66.1v59.4h66.1v-59.4zm-78.1-72.1h-66.1v60.1h66.1v-60.1z"/></svg>
    </a>
  
    
    
      
      
    
    <a href="https://twitter.com/cnych" target="_blank" rel="noopener" title="twitter.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg>
    </a>
  
    
    
      
      
    
    <a href="https://instagram.com/cnych" target="_blank" rel="noopener" title="instagram.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M224.1 141c-63.6 0-114.9 51.3-114.9 114.9s51.3 114.9 114.9 114.9S339 319.5 339 255.9 287.7 141 224.1 141zm0 189.6c-41.1 0-74.7-33.5-74.7-74.7s33.5-74.7 74.7-74.7 74.7 33.5 74.7 74.7-33.6 74.7-74.7 74.7zm146.4-194.3c0 14.9-12 26.8-26.8 26.8-14.9 0-26.8-12-26.8-26.8s12-26.8 26.8-26.8 26.8 12 26.8 26.8zm76.1 27.2c-1.7-35.9-9.9-67.7-36.2-93.9-26.2-26.2-58-34.4-93.9-36.2-37-2.1-147.9-2.1-184.9 0-35.8 1.7-67.6 9.9-93.9 36.1s-34.4 58-36.2 93.9c-2.1 37-2.1 147.9 0 184.9 1.7 35.9 9.9 67.7 36.2 93.9s58 34.4 93.9 36.2c37 2.1 147.9 2.1 184.9 0 35.9-1.7 67.7-9.9 93.9-36.2 26.2-26.2 34.4-58 36.2-93.9 2.1-37 2.1-147.8 0-184.8zM398.8 388c-7.8 19.6-22.9 34.7-42.6 42.6-29.5 11.7-99.5 9-132.1 9s-102.7 2.6-132.1-9c-19.6-7.8-34.7-22.9-42.6-42.6-11.7-29.5-9-99.5-9-132.1s-2.6-102.7 9-132.1c7.8-19.6 22.9-34.7 42.6-42.6 29.5-11.7 99.5-9 132.1-9s102.7-2.6 132.1 9c19.6 7.8 34.7 22.9 42.6 42.6 11.7 29.5 9 99.5 9 132.1s2.7 102.7-9 132.1z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.sections", "navigation.tabs", "navigation.tabs.sticky", "navigation.expand", "navigation.indexes", "navigation.top", "search.suggest", "search.highlight", "content.code.annotate"], "search": "../../../assets/javascripts/workers/search.01824240.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.config.lang": "ja", "search.config.pipeline": "trimmer, stemmer", "search.config.separator": "[\\uff0c\\u3002]+", "search.placeholder": "\u641c\u7d22", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version.title": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.e8a254df.min.js"></script>
      
        <script src="../../../js/extra.js"></script>
      
        <script src="../../../js/baidu-tongji.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
  </body>
</html>